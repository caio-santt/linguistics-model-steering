Um modelo do ambiente é apenas uma representação simplificada que permite ao agente prever, com certa precisão, o efeito de suas ações sobre o estado futuro. Em cenários de alta dimensionalidade, a construção de um modelo exato torna‑se impraticável, pois requer uma quantidade de dados e de estrutura computacional que cresce exponencialmente com o número de variáveis. Para contornar essa limitação, o aprendizado por reforço tem recorrido a duas estratégias complementares: a utilização de amostras de interação (exploração) e a adoção de funções de aproximação que generalizam o conhecimento adquirido em estados não vistos.

A primeira estratégia, chamada de “exploração por amostragem”, baseia‑se na premissa de que o agente não pode conhecer a dinâmica do ambiente de antemão, mas pode inferi‑la através da coleta de experiências. Cada interação (estado, ação, recompensa, próximo estado) é registrada em uma memória de replay ou em um buffer de experiência, permitindo que o agente recupere e reprocessar esses episódios de forma não sequencial. Essa técnica, combinada com algoritmos de aprendizado baseados em gradiente, como o Q‑learning ou o SARSA, possibilita a atualização de estimativas de valor que refletem a média das recompensas futuras esperadas.

A segunda estratégia, a aproximação de função, surge da necessidade de representar políticas ou funções de valor em espaços de estado muito grandes, onde armazenar valores tabulares seria inviável. Redes neurais profundas, por exemplo, podem ser treinadas para mapear estados de entrada (por exemplo, imagens de câmeras de um robô) para valores de ação ou para políticas diretas. No caso do Deep Q‑Network (DQN), uma rede neural aprende a estimar a função Q(s, a) a partir de amostras de experiência. A combinação de replay de memória e target networks estabiliza o processo de aprendizado, mitigando a correlação entre amostras consecutivas e a mudança de parâmetros da própria rede.

Além dos métodos baseados em Q‑learning, os algoritmos de política direta, como o REINFORCE ou os métodos de gradiente de política, abordam o problema de maneira distinta. Em vez de estimar valores de ação, esses métodos otimizam diretamente a política πθ(a|s) parametrizada por θ, maximizando a expectativa de retorno. Essa abordagem se mostra especialmente útil em ambientes com ações contínuas, onde discretizar o espaço de ações pode ser impraticável.

Um ponto crucial em todos esses métodos é a exploração versus exploração. Se o agente for excessivamente exploratório, ele gastará muito tempo em ações subótimas; se for excessivamente explorador, pode convergir para uma política subótima.