Na década de 1960, a IA passou a se firmar em novas direções, impulsionada pela crescente disponibilidade de hardware mais potente e pela emergência de novas teorias cognitivas. Um marco importante foi o desenvolvimento do *Logic Theorist* (1956) e do *General Problem Solver* (1957) por Newell e Simon, que demonstraram que algoritmos de busca e regras de inferência poderiam resolver problemas que antes eram considerados exclusivamente humanos. Esses sistemas, embora limitados em escala, abalaram a comunidade científica, pois mostraram que a lógica formal poderia ser implementada em máquinas.

Com o advento dos *expert systems* na década de 1970, a IA começou a se aproximar do mercado corporativo. O *MYCIN*, desenvolvido na Universidade de Stanford, era um programa que diagnosticamente via infecções bacterianas usando regras de produção. Embora não tenha sido comercializado, demonstrou o potencial de sistemas baseados em conhecimento para substituição de especialistas humanos em tarefas rotineiras. O sucesso de MYCIN inspirou a criação de outras aplicações, como o *XCON* (também conhecido como R1), que configurava sistemas de computadores em empresas de grande porte, economizando milhões de dólares em erros de configuração.

Simultaneamente, a teoria da *redes neurais* começou a ganhar atenção. A partir do modelo de perceptron de Rosenblatt (1958), pesquisadores perceberam que o aprendizado poderia ser distribuído em camadas de unidades de processamento simples. No entanto, a falta de métodos eficientes de treinamento e a crítica de Minsky e Papert em 1969 ("Perceptrons") desaceleraram o progresso. Foi somente com o avanço de algoritmos de retropropagação na década de 1980 que as redes neurais retomaram o protagonismo, permitindo a criação de classificadores mais complexos e a resolução de problemas de reconhecimento de padrões.

Na mesma época, a IA também se beneficiou da crescente interdisciplinaridade. A *cibernética* de Norbert Wiener continuou a influenciar a concepção de sistemas de controle autônomo, enquanto a *psicologia cognitiva* oferecia modelos de processamento de informação que poderiam ser formalizados em algoritmos computacionais. A convergência desses campos deu origem a abordagens mais holísticas, como a *IA simbólica* combinada com *IA conexionista*, buscando integrar regras explícitas e aprendizado estatístico.

A década de 1990 marcou uma nova fase, com o surgimento do *machine learning* baseado em dados em larga escala. A introdução de algoritmos de *support vector machines* (SVM) e *árvores de decisão* possibilitou a construção de modelos robustos para classificação e regressão, superando as limitações dos sistemas baseados apenas em regras.