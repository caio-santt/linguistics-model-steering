Alguns deles desenvolveram os primeiros modelos teóricos que ligavam a lógica formal ao raciocínio computacional, culminando em trabalhos que, em 1950, levaram à proposta de Alan Turing de um “máquina universal” capaz de simular qualquer algoritmo. Turing, ao escrever sobre a possibilidade de máquinas “pensarem”, estabeleceu o que viria a ser o teste de Turing, um marco conceitual que ainda hoje serve como referência para debates sobre consciência artificial. Em paralelo, no início dos anos 1950, John McCarthy, Marvin Minsky, Nathaniel Rochester e Claude Shannon organizaram a conferência de Dartmouth em 1956, onde o termo “inteligência artificial” foi cunhado e o campo emergiu como disciplina formal. A proposta de McCarthy de “programas que poderiam aprender” abriu caminho para os primeiros protótipos de sistemas de lógica de resolução de problemas, como o Logic Theorist e o General Problem Solver (GPS), ambos desenvolvidos por Allen Newell e Herbert A. Simon em 1957. Esses programas demonstraram que, ao representar conhecimento em forma simbólica e aplicar regras de inferência, uma máquina podia provar teoremas matemáticos e resolver problemas de lógica de forma automática, alimentando a crença de que a inteligência humana poderia ser emulado por algoritmos.

No entanto, a década de 1960 trouxe desafios que revelaram as limitações da abordagem puramente simbólica. A complexidade dos problemas do mundo real exigiu que os sistemas fossem capazes de lidar com incerteza e ambiguidade, áreas em que a lógica clássica falhava. Em resposta, surgiram os primeiros trabalhos sobre lógica probabilística e redes bayesianas, influenciados por economistas e estatísticos que buscavam modelos de decisão sob risco. Em 1965, Joseph Weizenbaum implementou o programa ELIZA, que simulava um psicoterapeuta, demonstrando que a interação natural entre humanos e máquinas poderia ser facilitada por técnicas de processamento de linguagem natural (PLN) baseadas em padrões de substituição e regras gramaticais. Embora ELIZA não entendesse o conteúdo, ela mostrava que a percepção de “inteligência” poderia ser manipulada por simples transformações linguísticas, levantando questões éticas sobre a representação e a confiança em sistemas automatizados.

Os anos 1970 e 1980 foram marcados por um ciclo de otimismo e desilusão, conhecido como “invernos da IA”. A escassez de poder computacional, combinada com a expectativa de que a lógica simbólica resolveria todos os problemas, levou a uma queda de financiamento e de interesse acadêmico. Contudo, durante esse período, surgiram os primeiros sistemas especialistas, como o MYCIN (1972), que aplicava regras de diagnóstico médico, e o XCON (1979), que configurava sistemas de computador.