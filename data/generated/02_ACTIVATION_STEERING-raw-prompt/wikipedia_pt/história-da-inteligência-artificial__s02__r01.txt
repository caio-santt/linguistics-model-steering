Na década de 1990, a IA começou a se deslocar de paradigmas simbólicos para abordagens estatísticas, impulsionada pelo surgimento de algoritmos de aprendizado de máquina mais robustos e pela disponibilidade crescente de dados digitais. O trabalho de Geoffrey Hinton, David Parker e outros na Universidade de Toronto sobre redes neurais recorrentes e o algoritmo de retropropagação (back‑propagation) permitiram que máquinas aprendam a partir de exemplos, em vez de serem programadas explicitamente para cada tarefa. Paralelamente, a técnica de máquinas de vetor de suporte (SVM), desenvolvida por Vladimir Vapnik e seus colaboradores, ofereceu uma solução elegante para problemas de classificação e regressão, consolidando-se como padrão de fato em competições de reconhecimento de padrões.

O avanço tecnológico nos anos 2000 foi marcado pela explosão de dados (“big data”) e pela disponibilidade de poder computacional em nuvem. Algoritmos como o Random Forest, introduzido por Leo Breiman, e a técnica de ensemble learning, ganharam popularidade por sua capacidade de combinar múltiplas árvores de decisão, reduzindo a variância e melhorando a generalização. Ao mesmo tempo, o surgimento de GPUs (Graphics Processing Units) como aceleradores de computação paralela possibilitou o treinamento de redes neurais profundas em escalas antes inimagináveis. Em 2012, o modelo AlexNet, desenvolvido por Alex Krizhevsky, Ilya Sutskever e Geoffrey Hinton, reduziu drasticamente a taxa de erro em imagens do conjunto ImageNet, marcando o início da era do deep learning.

Com o sucesso de redes convolucionais em visão computacional, a pesquisa em processamento de linguagem natural (NLP) também avançou rapidamente. O modelo Word2Vec, introduzido por Tomas Mikolov e equipe na Universidade de São Paulo, trouxe a ideia de representar palavras como vetores densos em espaços de alta dimensionalidade, capturando semânticas e relações sintáticas. Em 2014, a técnica de atenção (attention) foi formalizada em modelos de tradução automática neural, culminando na arquitetura Transformer, que eliminou a necessidade de recorrência e permitiu paralelização massiva. O BERT (Bidirectional Encoder Representations from Transformers), lançado em 2018, estabeleceu novos recordes em tarefas de compreensão de texto, como análise de sentimentos, resposta a perguntas e reconhecimento de entidades nomeadas.

A década de 2010 também testemunhou a popularização de algoritmos de aprendizado por reforço (RL). O trabalho de DeepMind, particularmente o algoritmo AlphaGo, demonstrou que máquinas poderiam dominar jogos complexos de estratégia, combinando redes neurais convolucionais com Monte Carlo Tree Search (MCTS).