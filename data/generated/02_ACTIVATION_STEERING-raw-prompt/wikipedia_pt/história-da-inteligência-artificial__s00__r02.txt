Alguns deles desenvolveram as primeiras máquinas de lógica simbólica, como o *Logic Theorist* de Allen Newell e Herbert A. Simon, que em 1955 demonstrou que um computador podia provar teoremas matemáticos. Em 1956, a conferência de Dartmouth, organizada por John McCarthy, Marvin Minsky, Nathaniel Rochester e Claude Shannon, consolidou a disciplina, definindo a IA como “a ciência e engenharia de fazer máquinas inteligentes”. McCarthy, que cunhou o termo “inteligência artificial”, já via a possibilidade de “programas que possam aprender, raciocinar e resolver problemas” como um objetivo realizável.

Na década seguinte, surgiram programas pioneiros como o *ELIZA* de Joseph Weizenbaum, que simulava um psicoterapeuta usando padrões de linguagem simples, e o *SHRDLU* de Terry Winograd, que manipulava objetos em um mundo de blocos usando linguagem natural. Esses sistemas mostraram que, embora limitados, as máquinas poderiam interagir de forma convincente com humanos, abrindo caminho para pesquisas em processamento de linguagem natural (PLN).

No início dos anos 70, a pesquisa em IA começou a se dividir em duas correntes principais: a lógica formal e os métodos estatísticos. Enquanto os primeiros buscavam raciocínio determinístico baseado em regras, os segundos exploravam probabilidades e aprendizado a partir de dados. O advento das redes neurais artificiais, inspiradas no modelo de Hodgkin-Huxley, trouxe uma nova perspectiva. Em 1986, David Rumelhart, Geoffrey Hinton e Ronald Williams introduziram o algoritmo de retropropagação, permitindo treinar redes com múltiplas camadas, um marco que revitalizou o interesse em modelos de aprendizado profundo.

Entretanto, a década de 80 também foi marcada por períodos de desânimo conhecidos como “invernos da IA”, quando expectativas grandiosas não se concretizaram. A falta de poder computacional, dados insuficientes e limitações de algoritmos levaram a cortes de financiamento e a um declínio temporário no entusiasmo. Ainda assim, projetos como o *XCON* da Digital Equipment Corporation mostraram que sistemas baseados em regras ainda eram úteis em ambientes corporativos.

Com o advento da internet e a explosão de dados digitais na década de 90, a IA encontrou novos alicerces. O aprendizado de máquina, especialmente os métodos de árvores de decisão e de máquinas de vetor de suporte (SVM), prosperou em tarefas de classificação e regressão. Paralelamente, o campo de PLN evoluiu com modelos baseados em n-gramas, permitindo sistemas de tradução automática e correção ortográfica mais precisos.

O início do século XXI trouxe uma revolução impulsionada pela disponibilidade de GPUs e pelo aumento de conjuntos de dados massivos.