… observabilidade completa. Quando essa condição não é satisfeita, o problema passa a ser um processo de decisão de Markov parcialmente observável (POMDP), no qual o agente recebe apenas observações O t que são funções probabilísticas do estado real S t. Nesses cenários, o agente precisa manter um histórico ou uma crença sobre o estado atual, tipicamente representada por uma distribuição de probabilidade sobre S t, e atualizar essa crença a cada nova observação e ação executada. A política, portanto, passa a depender dessa crença em vez do estado puro, tornando o problema significativamente mais complexo e exigindo técnicas de filtragem, como o filtro de Kalman estendido ou métodos de Monte Carlo para estimar a distribuição posterior.

Em ambientes contínuos, tanto os espaços de estado quanto os de ação são geralmente contínuos e de alta dimensionalidade. Para lidar com isso, os algoritmos de aprendizado por reforço evoluíram de métodos tabulares, que armazenam valores Q(s,a) em uma tabela, para representações funcionais que utilizam redes neurais, polinômios ou funções de base radiais. Essa abordagem, conhecida como aprendizado por reforço profundo, permite generalizar a partir de exemplos observados para estados ainda não visitados, reduzindo drasticamente a necessidade de exploração exaustiva.

A exploração versus exploração (exploit‑explore trade‑off) permanece um dos pilares conceituais do aprendizado por reforço. Estratégias clássicas, como ε‑greedy, softmax e UCB (Upper Confidence Bound), incentivam o agente a experimentar ações menos frequentes, equilibrando o ganho de recompensas imediatas com a descoberta de possíveis melhores caminhos. Em ambientes dinâmicos, métodos mais sofisticados, como o algoritmo de Thompson Sampling, aproveitam a incerteza nas estimativas de valor para guiar a exploração de maneira mais informada.

Quando o ambiente possui recompensas esparsas ou atrasadas, o agente pode sofrer com o problema de atribuição de crédito, pois não há feedback imediato que indique a utilidade de uma ação. Técnicas de atribuição de crédito, como o algoritmo de Monte Carlo, o método de diferenciação temporal (TD) e o algoritmo de Monte Carlo control variate, foram desenvolvidos para estimar o valor de uma política a partir de sequências de recompensas acumuladas.