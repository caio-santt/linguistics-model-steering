- Um modelo do ambiente é conhecido, permitindo que o agente simule futuras transições e avalie estratégias antes de executar ações no mundo real. Nessas situações, o aprendizado pode ser feito em modo “offline”, onde o agente explora a política ótima em um simulador antes de ser implantado.  
- Um modelo do ambiente é desconhecido, exigindo que o agente aprenda a partir de interações reais. Aqui entra em cena a exploração versus exploração: o agente deve experimentar ações que não lhe trazem recompensas imediatas, mas que podem revelar caminhos mais lucrativos a longo prazo.  

Quando o espaço de estados cresce, a representação explícita de cada par (estado, ação) torna-se impraticável. O uso de funções de aproximação—redes neurais, árvores de decisão, ou mesmo aproximações polinomiais—permite generalizar a partir de exemplos observados. Por exemplo, em um ambiente de controle de temperatura de um edifício, a rede pode mapear sensores de temperatura, umidade e horário para uma política que decide a potência do aquecedor. O aprendizado de tais redes, muitas vezes, requer técnicas de retropropagação e otimização estocástica, pois a recompensa é observada apenas após a execução de uma sequência de ações.

Em ambientes parcialmente observáveis, o agente pode manter um “estado interno” ou memória, usando, por exemplo, redes recorrentes ou modelos de filtro de Kalman, para inferir o estado real a partir de observações ruidosas. Essa memória torna a política dependente não apenas do estado atual, mas de todo o histórico, permitindo decisões mais informadas quando a observação direta é insuficiente.

A medição de desempenho em RL não se resume apenas à soma das recompensas recebidas. Conceitos como “valor futuro” (ou “valor esperado” de um estado) e “advantage” (vantagem) são cruciais. O algoritmo de Q‑learning, por exemplo, atualiza a estimativa de valor Q(s,a) pela diferença entre a recompensa imediata e a estimativa de valor do próximo estado, ponderada por uma taxa de aprendizado. Já o método de política gradiente otimiza diretamente a política πθ, ajustando os parâmetros θ na direção que aumenta a expectativa de recompensa.

Quando se fala de “regret” (arrependimento), a métrica avalia a diferença entre a soma das recompensas que o agente poderia ter obtido, se tivesse seguido a política ótima, e o que realmente obteve. Em cenários de aprendizado online, o objetivo é reduzir esse arrependimento ao longo do tempo, garantindo que a política converge para a ótima ou quase‑ótima.

Aplicações práticas ilustram a versatilidade do aprendizado por reforço.