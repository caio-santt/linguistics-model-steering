Quando a observabilidade é limitada, o agente não tem acesso ao estado completo \(s\), mas apenas a um vetor de observações \(o\).  Nesse cenário, a estrutura formaliza-se como um processo de decisão de Markov parcialmente observável (POMDP), no qual a transição e a recompensa dependem do estado oculto, enquanto o agente recebe apenas sinais que são probabilisticamente relacionados a ele.  A solução ótima requer, então, a manutenção de um *belief state*—uma distribuição de probabilidade sobre os estados possíveis—e a busca de políticas que maximizem a recompensa esperada condicionada a essa crença.

Para superar a necessidade de um modelo explícito do ambiente, os algoritmos de aprendizado por reforço foram desenvolvidos em duas grandes vertentes: métodos baseados em valores e métodos baseados em políticas.  Os primeiros, como o Q‑learning e o SARSA, aproximam o valor de ação \(Q(s,a)\) iterativamente, atualizando a estimativa com a diferença temporal (TD) entre a recompensa observada e o valor previsto.  Em ambientes com espaço de estados muito grande ou contínuo, a representação tabular torna‑se impraticável, exigindo técnicas de aproximação por funções.  Redes neurais feed‑forward, polinômios ou mesmo métodos de regressão por kernel são empregados para estimar \(Q(s,a;\theta)\), onde \(\theta\) denota os parâmetros do modelo.

A política, por outro lado, pode ser parametrizada diretamente, como em métodos de gradiente de política (REINFORCE, Actor‑Critic).  Nestes algoritmos, a política \(\pi(a|s;\phi)\) é otimizada em direção à maximização da recompensa cumulativa, calculando gradientes de expectativa de retorno em relação a \(\phi\).  A vantagem de se otimizar a política diretamente é a capacidade de lidar com ações contínuas e de incorporar funções de valor como critic, reduzindo a variância dos gradientes.

A exploração, crítica para evitar que o agente se prenda em estratégias subótimas, é tratada por diferentes esquemas.  Estratégias clássicas, como \(\epsilon\)-greedy ou softmax, introduzem aleatoriedade controlada nas escolhas de ação.  Em ambientes mais complexos, métodos de exploração por informação, como o *intrinsic motivation* ou *novelty search*, incentivam o agente a visitar estados pouco explorados, fornecendo recompensas internas além das externas.  A combinação dessas técnicas com aprendizado profundo resultou em algoritmos como o Deep Q‑Network (DQN), que utiliza redes convolucionais para processar entradas visuais e um *experience replay* para quebrar a correlação temporal entre amostras.