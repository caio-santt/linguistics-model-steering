O modelo de Q‑learning evolui rapidamente quando se introduzem técnicas de aproximação. Em vez de armazenar uma tabela completa, o agente pode usar redes neurais para estimar a função Q(s,a), permitindo lidar com espaços de estado contínuos e de alta dimensionalidade. Este paradigma, conhecido como Deep Q‑Network (DQN), combina o aprendizado por reforço com redes neurais profundas, permitindo que o agente aprenda políticas complexas a partir de entradas brutas, como pixels de imagens ou sinais sensoriais.

A construção de uma política ótima em ambientes estocásticos requer um delicado equilíbrio entre exploração e exploração. Estratégias como ε‑greedy, onde o agente escolhe ações aleatórias com probabilidade ε e a ação de maior valor estimado com probabilidade 1‑ε, são simples, mas eficazes. Métodos mais sofisticados, como Upper Confidence Bound (UCB) e Thompson Sampling, introduzem variáveis de confiança que penalizam a incerteza, incentivando o agente a explorar regiões pouco visitadas de forma mais sistemática.

Outra dimensão crucial é a definição de recompensas. Em muitos cenários, a recompensa pode ser escassa ou atrasada, dificultando a atribuição de mérito às ações individuais. Técnicas de shaping de recompensas, que introduzem sinais intermediários baseados em heurísticas, ajudam a acelerar o aprendizado, mas devem ser projetadas cuidadosamente para evitar enviesar o agente em direção a objetivos subótimos. O conceito de "reward shaping" formalizado por Ng, Harada e Russell garante que a estrutura de recompensas adicional preserve a política ótima, adicionando apenas um gradiente de valor que não altera a ordem de preferência entre políticas.

O aprendizado por reforço também se beneficia de abordagens de política direta, como Policy Gradient (PG). Em vez de estimar a função Q, o agente otimiza diretamente a política π(a|s;θ) parametrizada por θ, maximizando a recompensa esperada. O algoritmo REINFORCE, por exemplo, utiliza a estimativa de gradiente da expectativa de recompensa para atualizar os parâmetros. Embora seja simples de implementar, o algoritmo pode sofrer de alta variância. Métodos avançados, como Actor‑Critic, combinam um ator que propõe ações com um crítico que estima o valor de estado, reduzindo a variância e melhorando a estabilidade do aprendizado.

Para ambientes com grande número de estados, a decomposição de tarefas em sub‑tarefas hierárquicas tem se mostrado promissora. Hierarchical Reinforcement Learning (HRL) introduz níveis de abstração, onde um nível superior decide sobre meta‑ações que, por sua vez, são executadas por níveis inferiores. Isso não só reduz a complexidade do espaço de busca, mas também facilita a reutilização de políticas aprendidas em diferentes contextos.