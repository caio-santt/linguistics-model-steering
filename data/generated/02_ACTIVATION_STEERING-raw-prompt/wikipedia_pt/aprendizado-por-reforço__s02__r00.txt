- Um modelo do ambiente é desnecessário quando se recorre a métodos de aprendizado por amostragem, mas sua presença pode acelerar a convergência, especialmente em cenários com grande dimensionalidade. Em tais casos, técnicas de modelagem probabilística, como redes bayesianas dinâmicas ou modelos de Markov ocultos, são empregadas para capturar a dinâmica subjacente e permitir a simulação de cenários futuros sem a necessidade de interagir fisicamente com o ambiente real.

- Em ambientes parcialmente observáveis, a política ótima passa a depender de uma história de observações, não apenas do estado atual. Para lidar com essa dependência, as abordagens de memória, como redes recorrentes (RNN) ou LSTMs, são integradas ao agente, permitindo que ele mantenha um estado interno que funcione como um resumo compactado da trajetória observada. Alternativamente, técnicas de filtragem, como o filtro de Kalman estendido, podem ser empregadas para estimar o estado real a partir de observações ruidosas.

- O processo de otimização de políticas pode ser formulado em termos de gradientes de política, onde a derivada da expectativa de recompensa em relação aos parâmetros do modelo de política é estimada via amostras de trajetória. O algoritmo REINFORCE, por exemplo, utiliza o estimador de gradiente de Monte Carlo, multiplicando a recompensa cumulativa por cada passo pelo gradiente do logaritmo da probabilidade da ação escolhida. Para reduzir a variância desse estimador, métodos de baseline, como o uso de uma função de valor aproximada, são introduzidos, gerando o algoritmo Actor-Critic.

- A aproximação de função, seja por redes neurais profundas ou por funções baseadas em kernels, permite que o agente generalize entre estados semelhantes, mitigando o problema de explosão combinatória de estados. No contexto de Q-learning, a atualização de Bellman é estendida para o caso de aproximação contínua, resultando em algoritmos como Deep Q-Network (DQN). A técnica de replay de memória e a utilização de uma rede alvo estabilizam a aprendizagem, evitando ciclos de atualização que poderiam levar a divergência.

- A exploração versus exploração (exploration-exploitation) permanece um desafio central. Estratégias clássicas, como ε-greedy, decaem o fator ε ao longo do tempo, enquanto abordagens mais sofisticadas, como Upper Confidence Bound (UCB) ou Thompson Sampling, introduzem um elemento de incerteza na seleção de ações. No contexto de aprendizado profundo, métodos de exploração por perturbação de gradiente, como Randomized Value Functions, têm mostrado eficácia em ambientes de alta dimensionalidade.

- Em aplicações práticas, a robustez do agente frente a mudanças abruptas no ambiente é crucial.