1980 – Inteligência computacional  
Na década de 1980, a pesquisa em IA recebeu financiamento significativo da Agência de Projetos de Pesquisas Avançadas sobre Defesa (Defense Advanced Research Projects Agency – DARPA) nos Estados Unidos e do Projeto da Quinta Geração (Project 5 G) no Japão. Esse aporte impulsionou o desenvolvimento de sistemas especialistas, que buscavam codificar o conhecimento de especialistas humanos em regras explícitas. Projetos emblemáticos, como MYCIN (para diagnóstico de infecções bacterianas) e XCON (para configuração de sistemas de computador), demonstraram a viabilidade de aplicações comerciais de IA baseada em lógica de produção [10]. Entretanto, a dependência de regras estáticas e a dificuldade de manutenção dos bancos de conhecimento revelaram limitações fundamentais, levando à chamada “crise dos sistemas especialistas” no final dos anos 80 [11].

1990 – A ascensão da aprendizagem estatística  
Em resposta às falhas dos sistemas baseados em regras, pesquisadores voltaram sua atenção para abordagens estatísticas. O algoritmo de classificação de Máquinas de Vetores de Suporte (SVM), desenvolvido por Cortes e Vapnik em 1995, estabeleceu um novo paradigma para classificação e regressão em dados de alta dimensão [12]. Paralelamente, árvores de decisão e florestas aleatórias (Random Forests) de Breiman, introduzidas no início dos anos 90, ganharam popularidade devido à sua robustez e interpretabilidade [13]. O campo de aprendizagem de máquina (machine learning) consolidou-se como o núcleo da pesquisa em IA, deslocando a ênfase de regras explícitas para modelos probabilísticos e de otimização.

2000 – Renascimento das redes neurais  
A década de 2000 testemunhou o renascimento das redes neurais artificiais, impulsionado pelo algoritmo de retro‑propagação (back‑prop) e pela disponibilidade de grandes conjuntos de dados. Redes convolucionais (CNNs) de LeCun e seus colaboradores, aplicadas ao reconhecimento de dígitos manuscritos (MNIST), mostraram desempenho superior a métodos tradicionais [14]. Em 2006, Hinton e colaboradores introduziram a técnica de pré‑treino de camadas (pre‑training) em redes profundas, superando barreiras de treinamento e pavimentando o caminho para o que viria a ser chamado de deep learning [15].

2010 – IA no cotidiano e desafios éticos  
Com a disponibilidade de GPUs de alto desempenho, modelos como AlexNet (2012) e posteriormente VGG, ResNet e Inception estabeleceram novos recordes em benchmarks de visão computacional, como ImageNet [16]. A partir de 2014, o modelo Seq2Seq de atenção e, em 2017, o Transformer revolucionaram o processamento de linguagem natural, culminando em modelos generativos de grande escala como GPT‑3 e BERT [17].