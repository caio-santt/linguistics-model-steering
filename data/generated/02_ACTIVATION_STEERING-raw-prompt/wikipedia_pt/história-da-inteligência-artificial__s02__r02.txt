No final da década de 1980, a IA começou a ganhar novas formas de expressão, graças à ascensão dos algoritmos de aprendizado de máquina baseados em redes neurais profundas e à expansão dos recursos computacionais. O trabalho de Geoffrey Hinton, Yann LeCun e Yoshua Bengio, que introduziram as redes convolucionais e os autoencoders, trouxe um salto qualitativo na capacidade de reconhecer padrões visuais e auditivos. Enquanto isso, o algoritmo de aprendizado por reforço de Richard Sutton e Andrew Barto, que formalizou o paradigma de aprendizado por reforço, abriu caminho para agentes autônomos capazes de aprender interações complexas com ambientes dinâmicos.

Ao mesmo tempo, a comunidade de IA expandiu para além do escopo acadêmico, permeando indústrias que antes consideravam a tecnologia como mera curiosidade. Empresas de telecomunicações começaram a aplicar sistemas de diagnóstico baseados em lógica fuzzy para manutenção preditiva de redes de fibra óptica, enquanto bancos adotaram sistemas de detecção de fraude que combinavam regras de negócio com modelos estatísticos de classificação. Esse período de expansão industrial foi acompanhado por um aumento substancial de investimento público, especialmente em programas de financiamento europeu, como o “Horizon 2020”, que incentivou projetos de IA aplicada a saúde, transporte e energia.

A década de 1990 foi marcada por um fenômeno que muitos chamaram de “explosão de dados”. A popularização da internet, juntamente com a digitalização de documentos, gerou volumes de informação sem precedentes. Isso exigiu novos métodos de processamento e análise de dados, como o aprendizado de máquina supervisionado em larga escala e a mineração de dados em bases de conhecimento. O surgimento de bancos de dados semiestruturados, como XML e JSON, impulsionou o desenvolvimento de sistemas de recomendação, que se tornaram pilares de plataformas de comércio eletrônico e streaming.

Durante esse mesmo período, a IA se consolidou como disciplina interdisciplinar. A aplicação de técnicas de processamento de linguagem natural (NLP) em sistemas de diálogo avançou com o uso de modelos de linguagem baseados em n-gramas e, posteriormente, em representações vetoriais de palavras, como Word2Vec. Essas inovações possibilitaram a criação de assistentes virtuais que compreendem e respondem a consultas em linguagem natural, estabelecendo as bases para o que hoje chamamos de “inteligência conversacional”.

No início dos anos 2000, a IA enfrentou um novo desafio: a necessidade de integrar sistemas de IA em dispositivos móveis e embarcados. A limitação de recursos computacionais em smartphones exigiu a otimização de algoritmos e o desenvolvimento de modelos compactos, como as redes neurais quantizadas. Simultaneamente, a comunidade de IA começou a discutir questões éticas e de responsabilidade social, impulsionada por casos de viés em algoritmos de reconhecimento facial e de decisão automática em processos judiciais.