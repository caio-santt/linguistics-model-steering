Early history  
The first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel *Eugene Onegin* to illustrate the stochastic process. Though rudimentary, these chains demonstrated that linguistic structure could be captured by probabilistic models, laying a theoretical foundation for later statistical language models.

The 1950s and 1960s saw the emergence of rule‑based natural‑language generators, most notably ELIZA (1964) and PARRY (1972). ELIZA, designed by Joseph Weizenbaum at MIT, emulated a psychotherapist by pattern matching user input and generating canned responses. PARRY, built by Kenneth Colby, simulated a patient with paranoid schizophrenia, employing a more sophisticated state machine. These systems highlighted the potential of automated text generation, yet remained confined to narrow domains and heavily reliant on handcrafted rules.

In the 1970s and 1980s, the field shifted toward probabilistic approaches, spurred by the advent of the stochastic context‑free grammar (SCFG). Researchers such as Eugene Charniak and Mark Johnson explored statistical parsing and generation, producing rudimentary sentences that adhered to syntactic constraints. However, computational resources limited the scale of training data, and the output quality remained coarse compared to human writing.

The 1990s introduced the first neural language models. The n‑gram model, popularized by the work of Christopher Manning and others, captured local word dependencies and achieved modest improvements in speech recognition and machine translation. Yet, the curse of dimensionality persisted, as the vocabulary size grew and the model struggled to generalize beyond observed contexts.

The turn of the millennium marked a paradigm shift with the resurgence of deep learning. In 2006, Geoffrey Hinton and colleagues revived the deep belief network, demonstrating that hierarchical representations could be learned unsupervised. This breakthrough paved the way for recurrent neural networks (RNNs) and, later, long short‑term memory (LSTM) units, which addressed the vanishing gradient problem and enabled modeling of longer‑range dependencies. By 2010, the first large‑scale neural language models, such as the Google N‑gram dataset and the LSTM‑based neural machine translation system, began to outperform traditional statistical models on benchmark tasks.

The introduction of the transformer architecture in 2017, described in the seminal paper “Attention is All You Need” by Vaswani et al., revolutionized generative modeling. Transformers eschewed recurrence in favor of self‑attention mechanisms, allowing parallel computation over entire sequences and dramatically improving scalability.