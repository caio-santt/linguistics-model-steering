When the agent’s performance is compared to a benchmark policy, the metric of choice is often the expected return, defined as the discounted sum of future rewards. A discount factor, γ∈[0,1), controls the horizon over which the agent evaluates its actions, thereby balancing short‑term gains against long‑term benefits. In practice, γ is tuned to the time scale of the task: a value close to one encourages farsighted planning, while a smaller value makes the agent more myopic.

To estimate the value of a state, reinforcement learning relies on the Bellman equation, which recursively relates the value of a state to the values of its successors. When the transition dynamics are unknown, the agent must learn an approximation of this relationship from experience. Two broad families of algorithms address this learning problem. In model‑free methods, the agent updates a value function directly from sampled trajectories, whereas in model‑based approaches it first builds an approximate model of the environment and then solves it using dynamic programming techniques.

Temporal‑difference (TD) learning, exemplified by Q‑learning and SARSA, is the cornerstone of model‑free reinforcement learning. In Q‑learning, the agent maintains an estimate Q(s,a) of the expected return of taking action a in state s and following the current policy thereafter. Each interaction yields a TD error δ = r + γ maxₐ' Q(s',a') – Q(s,a), which is used to adjust Q(s,a). The learning rate α controls how aggressively the agent incorporates new information. SARSA, on the other hand, uses the action actually taken in the next state, δ = r + γ Q(s',a') – Q(s,a), making it on‑policy and more conservative in uncertain environments.

Policy‑gradient methods take a different route by parameterizing the policy πθ(a|s) directly. The objective is to maximize the expected return J(θ) = Eπθ[∑γⁿ rₙ]. The gradient ∇θJ(θ) is estimated via the REINFORCE algorithm, which samples trajectories and weights the gradient of the log‑policy by the cumulative reward. Actor‑critic architectures combine value estimation and policy improvement: the critic evaluates the value function V(s) or Q(s,a), while the actor updates the policy parameters in the direction suggested by the critic’s advantage estimate.

When the state space is large or continuous, function approximation becomes necessary. Linear function approximators, neural networks, or kernel methods can represent value functions or policies.