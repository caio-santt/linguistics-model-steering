[ 39 ] Markov decision processes (MDPs) provide a formalism for modelling sequential decision‑making under uncertainty.  An MDP is defined by a tuple (S, A, T, R, γ) where S is a set of states, A a set of actions, T(s, a, s′) the transition probability from s to s′ given a, R(s, a) the immediate reward, and γ the discount factor.  The optimal policy π∗ maximises the expected discounted return, which can be found by solving the Bellman optimality equations.  In practice, the state space of many real‑world problems is too large to enumerate, so approximate dynamic programming or reinforcement learning techniques are employed.

[ 40 ] Partially observable MDPs (POMDPs) extend the MDP framework to situations where the agent does not have perfect knowledge of the underlying state.  Instead of a state s, the agent receives an observation o with probability O(o | s′, a).  The belief state b(s) is a probability distribution over S that is updated via Bayesian filtering.  The optimal policy in a POMDP maximises expected return over belief states, but solving a POMDP exactly is PSPACE‑hard.  Practical solvers therefore use point‑based value iteration, policy‑gradient methods, or Monte‑Carlo tree search to approximate the value function over the continuous belief space.

[ 41 ] In many applications, the agent must balance exploitation (choosing actions that currently appear best) with exploration (gathering information that may improve future decisions).  Exploration bonuses, such as the intrinsic motivation signal of curiosity or information gain, are added to the reward function.  For instance, the Bayesian Information Gain (BIG) measures the expected reduction in entropy over the agent’s model parameters after observing a state transition.  An exploration‑exploitation trade‑off can then be formalised as maximizing the sum of extrinsic reward and λ·BIG, where λ controls the weight of intrinsic motivation.

[ 42 ] Hierarchical reinforcement learning (HRL) addresses the curse of dimensionality by decomposing a complex task into sub‑tasks.  Options or skills are temporally extended actions with their own policies and termination conditions.  The options framework defines a higher‑level MDP over options, enabling learning of a master policy that orchestrates lower‑level behaviors.  The MAXQ decomposition further breaks down the value function into additive components associated with each sub‑task, allowing efficient reuse of learned skills across different goals.