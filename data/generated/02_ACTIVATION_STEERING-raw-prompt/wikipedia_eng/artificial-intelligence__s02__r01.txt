A Markov decision process (MDP) formalises the trade‑off between action and consequence in a stochastic environment.  The world is represented as a set of states, a set of actions, a transition model that gives the probability of moving from one state to another when an action is taken, and a reward function that assigns a scalar value to each state or state‑action pair.  In a fully observable MDP the agent can always determine the current state and thus compute the optimal policy by dynamic programming or reinforcement learning.  However, in most real‑world scenarios the agent does not have perfect knowledge of the state.  The situation is then described by a partially observable Markov decision process (POMDP).  A POMDP augments the MDP with an observation model that maps hidden states to observable signals, and the agent must maintain a belief – a probability distribution over states – to make decisions.  Solving a POMDP exactly is NP‑hard, but a variety of approximation techniques (point‑based value iteration, policy search, Monte‑Carlo sampling) enable practical deployment in domains such as robotics, dialogue systems, and autonomous vehicles.

When the transition or observation models are not known a priori, the agent must learn them from data.  Model‑based reinforcement learning starts by collecting experience tuples (s, a, r, s′) and fitting a predictive model, often a neural network or a Bayesian linear regressor.  The learned model is then used to plan, either by Monte‑Carlo tree search or by solving the induced MDP.  Model‑free methods, such as Q‑learning or policy gradients, directly approximate the value or policy function without an explicit model.  In safety‑critical applications, one often combines a conservative model of the environment with a risk‑averse utility function, penalising trajectories that approach unsafe states even if they have high expected reward.

Exploration remains a central challenge.  An agent that simply follows the greedy policy will converge to a local optimum and miss higher‑reward strategies that require visiting rare states.  The intrinsic motivation framework introduces an auxiliary reward that quantifies the novelty or learning progress of a state.  The curiosity‑based signal can be computed as the prediction error of a forward dynamics model, the surprise of an observation model, or the reduction in uncertainty of a Bayesian belief.  By adding this intrinsic reward to the extrinsic task reward, the agent balances exploitation and exploration.  Information‑theoretic criteria, such as mutual information between actions and future observations, provide a principled way to quantify the value of information.