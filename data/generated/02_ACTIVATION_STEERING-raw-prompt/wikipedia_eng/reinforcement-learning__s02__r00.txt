In practice, the choice of learning algorithm is driven by the size of the state space, the smoothness of the reward landscape, and the computational budget available for training. For small tabular problems, classic dynamic programming techniques such as value iteration or policy iteration converge in a few passes over the state–action graph. When the state space is too large for an explicit table, one resorts to function approximation: a parametric mapping from states (and possibly actions) to value estimates or policy probabilities. Linear function approximators, kernel methods, or neural networks are common choices. The seminal work of Sutton and Barto on Temporal‑Difference learning demonstrated that bootstrapping the value of a state from the value of the next state yields a stable learning rule even when the environment is stochastic.

The exploration–exploitation dilemma is central to reinforcement learning. An agent must balance the immediate benefit of taking a known rewarding action against the long‑term gain of trying an unfamiliar action that might lead to a higher reward. ε‑greedy sampling, Boltzmann exploration, or upper‑confidence‑bound methods provide practical heuristics. In continuous action spaces, policy‑gradient methods such as REINFORCE or actor‑critic architectures allow the agent to adjust a stochastic policy directly, while deterministic policy gradients (DDPG) can exploit continuous control problems. The convergence guarantees of these algorithms depend on the smoothness of the policy manifold and the variance of the gradient estimates, which is why variance‑reduction techniques (e.g., baselines, advantage functions) are routinely incorporated.

Deep reinforcement learning (DRL) has revitalized the field by enabling agents to learn directly from high‑dimensional sensory inputs such as images or point clouds. The introduction of deep Q‑networks (DQN) showed that a convolutional network could approximate the action‑value function for Atari games, achieving superhuman performance on many titles. Subsequent improvements—experience replay, target networks, dueling architectures, prioritized replay—have addressed stability issues inherent in training with non‑stationary data. The policy‑gradient counterpart, Proximal Policy Optimization (PPO), has become a workhorse for continuous control tasks, balancing sample efficiency with robust convergence.

However, DRL still suffers from sample inefficiency, especially when the environment is costly to simulate or interact with. Model‑based approaches, where the agent learns an explicit transition model and plans ahead using techniques like Monte Carlo tree search, can dramatically reduce the number of real interactions required. Hybrid methods that combine model‑free policy learning with model‑based planning—such as Guided Policy Search or Dreamer—have shown promising results in robotics and simulated domains.