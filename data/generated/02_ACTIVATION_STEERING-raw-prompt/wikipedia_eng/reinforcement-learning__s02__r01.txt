In large, high‑dimensional settings the two pillars of modern reinforcement learning—sampling and function approximation—work in concert to tame otherwise intractable state spaces. First, sampling provides a data‑driven estimate of the transition dynamics and reward structure without requiring an explicit model. Second, function approximation, often realized through parametric families such as linear combinations of basis functions or deep neural networks, compresses the value or policy function into a tractable representation that generalises across unseen states.

These ingredients enable reinforcement learning to thrive in domains where the state space is astronomically large, the action space is continuous, or the environment is partially observable. Typical examples include:

* **Autonomous robotics** – A legged robot must learn to balance, walk, and navigate cluttered terrain. The raw proprioceptive and exteroceptive signals form a continuous state space that is too large for tabular methods. A deep Q‑network or an actor‑critic architecture, trained on thousands of simulated roll‑outs, learns a policy that maps high‑dimensional sensor readings to motor torques. Because the robot’s sensors are noisy and occluded, the underlying problem is a partially observable MDP, and recurrent or belief‑state approximations are often employed to maintain a history of observations.

* **Strategic games** – In chess or Go, the branching factor grows combinatorially with the depth of search. Monte‑Carlo Tree Search combined with a neural network that evaluates board positions (as in AlphaZero) turns the combinatorial explosion into a tractable optimisation problem. The network learns a value function and a policy that guides the search, effectively reducing the search space to the most promising lines.

* **Energy management** – Smart grids must decide how much energy to store, generate, or consume at each time step, given stochastic demand and renewable supply. The state includes continuous variables such as battery charge levels, weather forecasts, and market prices. Deep deterministic policy gradients (DDPG) or proximal policy optimisation (PPO) can learn a policy that maximises long‑term profit while respecting physical constraints like battery degradation limits.

* **Finance and portfolio optimisation** – Agents must allocate capital across a large universe of assets, balancing expected returns against risk. The state comprises market indicators, macro‑economic variables, and the agent’s own holdings. Because the market dynamics are partially observable and non‑stationary, online learning algorithms that adapt to shifting regimes are essential. Policy gradient methods that optimise a stochastic policy can capture the uncertainty inherent in financial decision‑making.

* **Healthcare treatment planning** – Sequential decision‑making is central to chronic disease management, where each treatment choice affects future health states and costs.