When the agent’s performance is compared to that of a hand‑crafted policy or a random baseline, a variety of quantitative criteria can be employed.  The most common is the cumulative return, defined as the sum of discounted rewards accrued over an episode, \(G_t=\sum_{k=0}^{\infty}\gamma^k r_{t+k}\), where \(\gamma\in[0,1)\) is the discount factor.  The expected return under policy \(\pi\) is denoted \(V^\pi(s)=\mathbb{E}_\pi[G_t|s_t=s]\) and the optimal value function \(V^*(s)=\max_\pi V^\pi(s)\).  By comparing \(V^\pi(s)\) to \(V^*(s)\) one obtains a bound on suboptimality that is independent of the particular trajectory taken.  Alternative metrics include average‑reward formulations, sample‑efficiency curves, and convergence diagnostics such as the variance of the return or the entropy of the policy distribution.

In practice, the evaluation protocol typically involves a fixed set of test environments that are drawn from the same distribution as the training set but are otherwise unseen.  For episodic tasks, the agent is run for a predetermined number of episodes, and the mean and standard deviation of the return are reported.  In continuous‑control domains, one often reports the mean squared error between the agent’s action distribution and a ground‑truth expert policy, or the time‑to‑goal metric in navigation tasks.  When the state space is partially observable, performance is additionally assessed via the agent’s ability to maintain a belief over hidden variables, measured by the Kullback–Leibler divergence between the inferred belief and the true hidden state distribution.

A key challenge in reinforcement learning is the exploration–exploitation trade‑off.  Early algorithms such as \(\epsilon\)-greedy Q‑learning [6] introduce stochasticity in action selection to encourage exploration, whereas more sophisticated approaches, such as Upper Confidence Bound (UCB) methods [7] or Thompson sampling [8], incorporate uncertainty estimates directly into the action‑selection rule.  In partially observable settings, belief‑state exploration strategies, for example, maintain a particle filter over latent states and use information‑theoretic criteria to decide when to gather additional observations.  Recent work on intrinsic motivation [9] augments the reward signal with novelty bonuses, thereby shaping the policy toward unfamiliar states without explicit knowledge of the transition dynamics.

The choice of function approximator has a profound impact on both learning dynamics and generalization.