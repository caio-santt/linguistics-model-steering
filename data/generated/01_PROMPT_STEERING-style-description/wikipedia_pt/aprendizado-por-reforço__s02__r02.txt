Um modelo do ambiente é uma representação matemática que descreve como o estado muda quando uma ação é tomada. Ele permite prever o próximo estado e a recompensa que virá. Quando o agente tem um modelo, ele pode simular várias ações antes de executar. Assim, ele economiza amostras reais e aprende mais rápido. Mas nem sempre o modelo está disponível. Às vezes, o ambiente é desconhecido ou muito complexo. Nesses casos, o agente deve aprender apenas a partir das interações. Ele observa, tenta, falha e tenta novamente. Essa é a essência do aprendizado por reforço sem modelo.

Para lidar com ambientes de grande escala, precisamos de aproximação de função. Em vez de armazenar uma tabela de valores para cada estado e ação, usamos uma função que estima esses valores a partir de características. Essa função pode ser linear ou não linear. Redes neurais são a escolha mais comum hoje. Elas capturam padrões complexos. Elas também permitem generalizar para estados nunca vistos. O agente treina a rede com amostras coletadas. Cada amostra atualiza os pesos. Assim, a política evolui continuamente.

Quando o agente usa aproximação, ele também precisa lidar com exploração. Ele deve experimentar ações novas para descobrir se há recompensas maiores. O algoritmo epsilon-greedy escolhe a ação ótima na maioria das vezes, mas com probabilidade epsilon escolhe aleatoriamente. Isso mantém a exploração. O valor de epsilon diminui ao longo do tempo. Assim, o agente explora no início e explora menos no fim.

Outra técnica é a política baseada em gradiente. Em vez de estimar valores, o agente ajusta diretamente a política. Ele usa o gradiente da recompensa esperada. Esse método funciona bem em espaços contínuos. Ele evita a necessidade de discretizar ações. O algoritmo REINFORCE é um exemplo clássico. Ele calcula o retorno total a partir de cada passo e usa esse retorno para ajustar a política.

O aprendizado por reforço também pode ser aplicado a tarefas com observabilidade parcial. Quando o agente não vê o estado completo, ele deve inferir informações ocultas. Ele pode usar memória recorrente, como redes LSTM, para manter um histórico de observações. Assim, ele lembra padrões que indicam o estado verdadeiro. Essa abordagem ajuda em jogos de cartas, onde as cartas ocultas influenciam a estratégia.

Em ambientes de alta dimensionalidade, a exploração torna-se mais difícil. O agente pode usar técnicas de amostragem para reduzir a variância. O algoritmo Monte Carlo Tree Search (MCTS) expande um pequeno número de nós em cada passo. Ele usa simulações para avaliar ações. O AlphaGo combinou MCTS com redes neurais para vencer humanos no Go. Esse sucesso mostrou que a combinação de aprendizado por reforço com busca deliberada pode superar limites humanos.