Pa ( s , s′ ) = Pr ( St+1 = s′ | St = s , At = a ) , a probabilidade de transição ( no tempo t ) de estado s para estado s′ sob a ação a. O agente escolhe ações com base em uma política π (s) que mapeia estados para ações. O objetivo é maximizar a soma de recompensas r_t. A recompensa pode ser imediata ou atrasada. O algoritmo Q‑learning atualiza Q(s,a) usando a regra: Q ← Q + α[r + γ max_a' Q(s',a') – Q]. O algoritmo funciona sem modelo. Ele explora e explora. A exploração escolhe ações aleatórias. A exploração usa a política atual. O equilíbrio é controlado por ε. Quando ε é alto, o agente explora. Quando ε diminui, o agente explora menos. O aprendizado acontece em passos. Cada passo envolve observar estado, escolher ação, receber recompensa, observar próximo estado, atualizar Q.