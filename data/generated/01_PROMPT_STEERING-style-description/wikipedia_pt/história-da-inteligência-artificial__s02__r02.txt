Na década de 1980 , a pesquisa em inteligência artificial recebeu financiamento significativo da Agência de Projetos de Pesquisas Avançadas sobre Defesa ( DARPA ) , nos Estados Unidos , e do Projeto da Quinta Geração , no Japão .

Os anos seguintes viram a explosão de sistemas especialistas. Empresas lançaram programas que simulavam médicos, engenheiros e advogados. Esses sistemas operavam em bases de conhecimento estáticas. Eles consultavam regras e retornavam respostas rápidas. O público via esses programas como promessas de revolução. Contudo, a complexidade do conhecimento humano mostrava limites. As regras se multiplicavam. O custo de manutenção crescia rapidamente. A maioria dos projetos parou de receber fundos. O entusiasmo inicial esgotou-se. Em 1987 , a DARPA criou o programa “Deep Thought” para testar algoritmos de busca em jogos. O projeto falhou em superar o nível de jogadores humanos. A comunidade refletiu sobre a necessidade de novas abordagens.

Pouco depois, os pesquisadores voltaram a olhar para redes neurais. A década de 1990 trouxe o algoritmo de retropropagação, que permitia treinar redes com múltiplas camadas. Os cientistas começaram a testar essas redes em reconhecimento de padrões. Os resultados foram surpreendentes. A precisão aumentou em dezenas de pontos percentuais. Em 1998 , o algoritmo de aprendizado profundo “AlexNet” reduziu a taxa de erro em competição de imagens. Esse avanço marcou o início da era de aprendizado profundo. As redes passaram a processar dados em grande escala. A velocidade de computação aumentou rapidamente graças às GPUs.

Ao mesmo tempo, surgiram novas áreas de aplicação. A visão computacional evoluiu. Os carros começaram a reconhecer sinais de trânsito. Em 2004 , o projeto DARPA Grand Challenge lançou uma corrida de veículos autônomos. Os carros competiram em estradas abertas. Alguns chegaram perto de completar o percurso. A experiência demonstrou que a IA podia operar em ambientes reais. A comunidade aumentou a confiança em sistemas autônomos.

No campo do processamento de linguagem natural, os modelos de linguagem ganharam notoriedade. Em 2003 , o algoritmo “Latent Semantic Analysis” melhorou a busca por textos. Em 2013, o modelo “Word2Vec” permitiu representar palavras em vetores contínuos. Esses vetores capturaram relações semânticas. Em 2018, o modelo “BERT” alcançou desempenho recorde em tarefas de compreensão de texto. A IA passou a entender contextos complexos. Os usuários começaram a interagir com assistentes virtuais. As conversas tornaram-se mais naturais.