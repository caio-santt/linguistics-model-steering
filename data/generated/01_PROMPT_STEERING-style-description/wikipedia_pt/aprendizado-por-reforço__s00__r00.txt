We need to continue the text in Portuguese. Use short sentences, simple syntax, high verb density, dynamic, introspective tone, continuous flow. About 590 tokens. Continue from the last incomplete sentence: "Pa(s, s′) = Pr(St+1 = s′ | St = s, At = a), a probabilidade de transição (no tempo t) de estado s para estado s′ sob". We need to continue the description of the reinforcement learning framework. Must keep the same style. Provide about 590 tokens (~ 590 words). Portuguese, short sentences. Avoid adjectives, keep action oriented. Let's produce about 590 tokens. We'll need to count approximate tokens. Roughly 1 token per word. 590 words. We'll write about 590 words. Let's aim for about 590 words. We'll produce about 590 words. Let's write.

We must not repeat introductory text. Just continue. Use simple sentences, high verb density, introspective, continuous flow. Let's produce.

We need to keep the same tone: introspective, continuous flow. Use short sentences. Let's produce about 590 words. We'll count roughly. Let's write about 590 words.

We start from "Pa(s, s′) = Pr(St+1 = s′ | St = s, At = a), a probabilidade de transição (no tempo t) de estado s para estado s′ sob". Continue.

Let's write: "ação a. O agente observa s, escolhe a, recebe recompensa r, entra em s′. O objetivo é maximizar R = Σγ^t r_t. A política π determina a escolha de ações. A política pode ser determinística ou estocástica. O aprendizado busca π* que maximize R. Para isso, calculamos Q(s,a), a utilidade esperada. Usamos Bellman: Q(s,a) = E[r + γ max_{a'} Q(s',a')]. Em Q-learning, atualizamos Q(s,a) = Q(s,a) + α [r + γ max_{a'} Q(s',a') - Q(s,a)]. O algoritmo explora, escolhe aleatoriamente, e explora, segue π. A exploração diminui com ε. O algoritmo convergente se α decresce. Em ambientes contínuos, usamos redes neurais para aproximar Q. O DQN faz isso. O agente interage, grava transições em replay buffer. Em seguida, treina a rede. O treinamento reduz a perda MSE entre Q(s,a) e target. A política derivada da rede escolhe argmax_a Q(s,a). Em ambientes com grande estado, a generalização é crucial. O aprendizado por reforço funciona em jogos, robótica, finanças. Em jogos, o agente aprende a vencer. Em robótica, o agente aprende a caminhar.