Observabilidade total não costuma ocorrer. Em ambientes reais, o agente vê apenas sinais ruidosos. Ele deve inferir o estado oculto. Isso gera problemas de decisão em cadeia de estados ocultos. Modelos ocultos de Markov são usados. Eles descrevem transições de estado e observações. O agente aprende a estimar o estado atual. Em seguida, escolhe a melhor ação. O algoritmo de filtro de Kalman ajuda. Em ambientes discretos, o filtro de partículas funciona bem. Ele mantém uma população de hipóteses. Cada hipótese representa um possível estado. O filtro atualiza as hipóteses com cada nova observação. O agente então calcula a política. A política pode ser determinística ou estocástica. A maioria dos métodos usa estimativas de valor. O algoritmo Q‑learning atualiza valores Q. Ele observa o par (s, a, r, s'). A atualização ocorre por: Q(s,a) ← Q(s,a) + α [r + γ max Q(s',a') – Q(s,a)]. Esse procedimento converge em ambientes finitos. O algoritmo SARSA atualiza usando a ação real escolhida. Ele segue: Q(s,a) ← Q(s,a) + α [r + γ Q(s',a') – Q(s,a)]. A diferença entre Q‑learning e SARSA é que o primeiro usa a ação ótima em s', enquanto o segundo usa a ação real. Ambos requerem exploração. A estratégia ε‑greedy é comum. O agente escolhe aleatoriamente com probabilidade ε. Caso contrário, escolhe a ação de maior valor. Em ambientes com grande espaço de estado, as tabelas Q tornam‑se impraticáveis. Redes neurais paramétricas substituem tabelas. O algoritmo Deep Q‑Network (DQN) usa uma rede para aproximar Q. Ele treina a rede com retropropagação. A perda é a diferença entre valor alvo e estimado. O alvo usa a própria rede. O replay buffer armazena transições. O agente amostra mini‑lotes aleatórios. Isso reduz correlação temporal. O algoritmo Actor‑Critic combina política e valor. O ator escolhe ações. O crítico avalia. O gradiente de política é estimado. O algoritmo Proximal Policy Optimization (PPO) limita grandes atualizações. Ele usa clipping. O resultado é estabilidade. Em ambientes contínuos, métodos de política determinística funcionam. O algoritmo DDPG combina Q‑learning e política. Ele usa duas redes: ator e crítico. O ator gera ações contínuas. O crítico avalia. Em aprendizado por reforço profundo, a arquitetura de rede importa. Convoluções extraem feições visuais. Recorrência captura dependência temporal. Em ambientes com observação parcial, o agente pode usar memória.