Formular o problema como um processo de decisão de Markov pressupõe que o agente observa diretamente o estado atual do ambiente ; nesse caso , diz‑se que o problema tem observabilidade completa. Quando a observação não é direta, chamamos de processo de decisão oculto. O agente então trabalha com estimativas, usando filtros ou redes neurais. Ele tenta inferir o estado real a partir de observações ruidosas. Isso aumenta a complexidade do algoritmo. A política então depende de um histórico de observações, não apenas do estado atual. O aprendizado torna‑se mais lento. O agente precisa explorar mais. Ele tenta ações diferentes para descobrir o que acontece. Cada ação gera uma nova observação. O agente acumula conhecimento. Ele atualiza suas crenças sobre o mundo. Isso é chamado de aprendizado de modelo. O modelo pode ser interno ou externo. Em muitos casos, o agente não tem acesso ao modelo. Ele então aprende diretamente pela interação. A política pode ser tabular ou aproximada. As tabelas são simples, mas limitadas. Quando o espaço de estados cresce, as tabelas ficam enormes. Então usamos aproximações. Redes neurais são comuns. Elas podem mapear observações para ações. A rede aprende por gradiente. O agente faz ajustes nos pesos. Cada episódio ajuda a refinar a política. A política pode ser determinística ou estocástica. Em ambientes ruidosos, a estocasticidade ajuda. O agente pode escolher diferentes ações em situações semelhantes. Isso aumenta a robustez. A recompensa pode ser imediata ou atrasada. Quando atrasada, o agente precisa lembrar de eventos passados. Isso requer memória. As redes recorrentes ajudam. Elas guardam histórico. O agente então prevê recompensas futuras. O objetivo final é maximizar a soma descontada. O desconto penaliza recompensas distantes. Isso incentiva o agente a agir cedo. O algoritmo de Q‑learning é popular. Ele atualiza valores Q(s,a). A fórmula simples: Q ← Q + α [R + γ max_a' Q(s',a') - Q]. A taxa α controla a velocidade de aprendizado. O fator γ controla o horizonte. Se γ = 0, o agente ignora o futuro. Se γ próximo de 1, o agente considera longas sequências. Em ambientes contínuos, usamos DQN. A rede estimula valores Q. A política escolhe a ação com maior Q. Em alguns casos, usamos policy gradients. Esses métodos atualizam a política diretamente. A vantagem é que a política pode ser suave. Em outros, usamos actor‑critic. O ator escolhe ações. O crítico avalia. Isso melhora a estabilidade. Em aprendizado profundo, a exploração é crítica. Métodos como ε‑greedy ou softmax ajudam.