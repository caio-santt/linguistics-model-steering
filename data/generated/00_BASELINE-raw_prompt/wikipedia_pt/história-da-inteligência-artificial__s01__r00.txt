O sucesso inicial prosseguiu com a criação de programas simples que já demonstravam a viabilidade de simular raciocínio lógico. Em 1956, o algoritmo de busca em árvore de depth-first, chamado “Logic Theorist”, conseguiu provar teoremas matemáticos de maneira automática, provando que a máquina poderia, em princípio, replicar a lógica humana. Logo depois, o “General Problem Solver” (GPS), desenvolvido por Newell e Simon, utilizou heurísticas para resolver problemas de lógica e geometria, sendo a primeira tentativa de construir um “cérebro universal” capaz de abordar qualquer tarefa de resolução de problemas.

Entretanto, o entusiasmo foi rapidamente temperado pela realidade de limitações técnicas. Os computadores de época eram enormes, lentos e consumiam energia exorbitante. A memória era escassa e o custo de fabricação de hardware era proibitivo. Além disso, a teoria da inteligência artificial ainda não tinha um modelo matemático robusto para aprender com dados, o que limitava a capacidade de adaptação das máquinas. Em 1969, o projeto “Shakey”, da Carnegie Mellon, marcou um avanço na robótica autônoma, mas seu desempenho era limitado a ambientes controlados e suas decisões eram baseadas em regras predefinidas.

A década de 1970 trouxe um período de introspecção e, em certa medida, de desânimo. O chamado “Inverno da IA” foi caracterizado por cortes de financiamento e expectativas não atendidas. Os pesquisadores começaram a perceber que a inteligência humana não se reduz apenas a regras lógicas, mas envolve percepção, aprendizado, linguagem e emoção. Nesse contexto, emergiram novas abordagens, como a lógica fuzzy, que permitia lidar com incertezas e valores intermediários, e o aprendizado de máquina supervisionado, que introduziu a ideia de que as máquinas poderiam aprender a partir de exemplos.

Ao mesmo tempo, a teoria da informação de Claude Shannon e a cibernética de Norbert Wiener foram revisitadas, gerando um interesse renovado em redes neurais artificiais. No início dos anos 80, o “Perceptron” de Frank Rosenblatt, embora limitado a problemas lineares, abriu caminho para o desenvolvimento de redes multicamadas. A introdução de algoritmos de retropropagação, em 1986, permitiu treinar redes com múltiplas camadas ocultas, superando o problema de separabilidade linear. Essa descoberta foi um divisor de águas, pois as redes neurais passaram a ser vistas como ferramentas viáveis para modelar funções complexas.

Enquanto isso, o campo da visão computacional avançava com a aplicação de transformadas de Fourier e técnicas de filtragem, permitindo que os computadores interpretassem imagens básicas. Em 1989, o algoritmo de detecção de bordas de Canny foi implementado em sistemas de visão, abrindo caminho para aplicações em robótica e reconhecimento de padrões.