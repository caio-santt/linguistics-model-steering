Um modelo do ambiente é útil quando a dinâmica real pode ser simulada com precisão suficiente, permitindo que o agente experimente estratégias sem custo real. Nesse cenário, a aprendizagem por reforço pode ser dividida em duas categorias principais: métodos baseados em modelo, que constroem explicitamente uma representação probabilística ou determinística das transições e recompensas, e métodos sem modelo, que operam diretamente com experiências observadas, atualizando estimativas de valor ou políticas sem conhecer a estrutura interna do ambiente.

Em ambientes com grande dimensionalidade de estado ou ação, a representação explícita de todas as transições torna-se impraticável. Para superar essa barreira, aproximadores de função, como redes neurais profundas, polinômios de Fourier ou máquinas de vetor de suporte, são empregados para generalizar a partir de exemplos limitados. Esses aproximadores permitem que o agente aprenda uma estimativa contínua da função de valor \(V(s)\) ou da ação‑valor \(Q(s,a)\), reduzindo drasticamente o número de parâmetros necessários e possibilitando a aplicação de RL em tarefas complexas, como controle de robôs em espaços de movimento contínuo ou em jogos com grandes espaços de estado, como o Go.

A escolha entre aprendizado por reforço on‑policy e off‑policy depende das exigências de exploração e estabilidade do algoritmo. Métodos on‑policy, como o SARSA (State‑Action‑Reward‑State‑Action), atualizam a política com base nas ações reais tomadas pelo agente, garantindo que a política seja sempre a mesma que está sendo avaliada. Em contraste, métodos off‑policy, como Q‑learning, permitem que o agente aprenda a política ótima enquanto segue uma política exploratória, o que pode acelerar a convergência em ambientes com grande número de ações.

Para lidar com a exploração, algoritmos modernos introduzem técnicas de exploração adaptativa, como o uso de distribuição de probabilidade sobre ações (por exemplo, softmax) ou a introdução de ruído gaussiano em redes neurais. Em ambientes de alta dimensionalidade, a exploração pode ser guiada por estratégias de busca em árvore, como Monte Carlo Tree Search (MCTS), que combina simulações de jogo com estimativas de valor aproximadas por redes neurais. Essa abordagem foi fundamental para o sucesso do AlphaGo, que utilizou MCTS em conjunto com redes neurais convolucionais para avaliar posições de tabuleiro e selecionar jogadas.

Além disso, a robustez do agente frente a mudanças no ambiente pode ser aumentada através de técnicas de regularização e de treinamento em múltiplas simulações. O algoritmo de aprendizado por reforço profundo, Deep Q‑Network (DQN), introduziu a ideia de replay de memória, permitindo que o agente reutilize experiências passadas, reduzindo a correlação entre amostras e melhorando a eficiência do aprendizado.