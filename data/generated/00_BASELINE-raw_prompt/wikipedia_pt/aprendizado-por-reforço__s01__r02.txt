Formular o problema como um processo de decisão de Markov pressupõe que o agente observa diretamente o estado atual do ambiente; nesse caso, diz-se que o problema tem observabilidade plena. Quando a observação é parcial ou ruidosa, a estrutura muda para um processo de decisão de Markov oculto (POMDP), onde o agente recebe apenas uma distribuição de probabilidade sobre o estado real. Essa diferença implica que a política ótima deve ser expressa em termos de crença (belief state) – uma distribuição sobre o espaço de estados – em vez de uma ação condicional ao estado observável.

Para lidar com observabilidade parcial, vários métodos foram desenvolvidos. Uma abordagem clássica é a “heurística de crença”, que mantém e atualiza a distribuição de crença a cada passo usando a regra de Bayes. A política resultante, então, é otimizada em cima desse espaço de crença, mas a dimensionalidade explora exponencialmente, tornando a solução exata impraticável para problemas de tamanho moderado. Para contornar isso, técnicas de aproximação, como o algoritmo de Partially Observable Monte Carlo Planning (POMCP), utilizam amostragem de partículas para representar a crença e simulam futuros trajetos de forma estocástica, permitindo a execução em tempo real mesmo em cenários complexos.

Outra linha de pesquisa concentra-se em arquiteturas de rede neural que aprendem a mapear observações ruidosas diretamente em ações, sem explicitamente estimar a crença. As redes recorrentes, como LSTMs ou GRUs, são capazes de acumular informação ao longo do tempo, atuando como um filtro interno que tenta reconstruir o estado oculto a partir da sequência de observações. Essa abordagem foi popularizada por trabalhos que combinam aprendizado profundo com aprendizado por reforço, como Deep Recurrent Q-Networks (DRQN) e Deep Deterministic Policy Gradient (DDPG) com memória recorrente. A vantagem é que o agente não precisa de um modelo explícito do ambiente, apenas de um mecanismo que capture dependências temporais.

Em cenários de controle contínuo, a teoria de controle ótimo se alia ao aprendizado por reforço por meio de métodos de controle adaptativo. O controle de reforço de modelo livre (Model-Free RL) pode ser interpretado como a busca de políticas que minimizem uma função de custo cumulativo, semelhante à abordagem de controle ótimo de LQR (Linear Quadratic Regulator). No entanto, enquanto o LQR exige conhecimento linear do modelo e ruído gaussiano, o RL pode operar em domínios não-lineares e com recompensas arbitrárias.