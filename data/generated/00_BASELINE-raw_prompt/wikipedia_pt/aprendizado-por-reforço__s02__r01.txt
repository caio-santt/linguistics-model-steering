Um modelo do ambiente é necessário quando a transição de estado não pode ser observada diretamente, exigindo que o agente aprenda a estimar as probabilidades de transição e as recompensas associadas. Nessas situações, técnicas de modelagem, como redes neurais recorrentes ou árvores de decisão, são empregadas para aproximar a dinâmica do sistema, permitindo que o agente execute simulações internas e planeje ações de forma mais eficaz. A abordagem de modelagem também abre caminho para o aprendizado por imitação, onde o agente observa demonstrações humanas ou de especialistas e tenta replicar o comportamento, reduzindo o espaço de busca inicial e acelerando a convergência.

Para ambientes com dimensões de estado muito altas, a representação explícita de todas as possibilidades torna-se impraticável. Aqui entra em cena a aproximação de função, que permite que o agente generalize a partir de amostras limitadas. Métodos de aproximação linear, como o método de gradiente temporal (TD) e o algoritmo Q‑learning, já foram extensivamente utilizados para resolver problemas de controle em espaços discretos. Entretanto, quando o espaço de ação continua ou o estado apresenta uma estrutura complexa, as redes neurais profundas se mostram indispensáveis. O algoritmo Deep Q‑Network (DQN), por exemplo, combina a técnica de replay de memória com uma rede convolucional para estimar a função Q, superando limitações de memória e permitindo a aprendizagem em jogos de vídeo com alta dimensionalidade de entrada.

Em cenários onde a recompensa é escassa ou altamente descontada, o trade‑off entre exploração e exploração se torna crítico. Estratégias de exploração, como ε‑greedy, softmax ou UCB (Upper Confidence Bound), incentivam o agente a experimentar ações pouco conhecidas, enquanto métodos de exploração baseada em informação, como o algoritmo Bayesian Optimization, avaliam a incerteza nas estimativas de valor. A introdução de terminação de episódio aleatória, ou o uso de recompensas de longo prazo ponderadas por fator de desconto γ, ajuda a equilibrar a busca por recompensas imediatas e a maximização de ganhos futuros.

Outra vertente promissora é o aprendizado por reforço multi‑agente, onde vários agentes interagem em um ambiente compartilhado. A cooperação e competição entre eles geram dinâmicas sociais complexas, exigindo algoritmos de aprendizado distribuído e comunicação eficiente. Protocolos de aprendizado federado permitem que agentes aprendam de forma assíncrona, preservando privacidade e reduzindo a sobrecarga de comunicação. Em aplicações de logística, por exemplo, múltiplos robôs de armazém coordenam suas rotas para otimizar a entrega de mercadorias, enquanto evitam colisões e minimizam o tempo total de operação.