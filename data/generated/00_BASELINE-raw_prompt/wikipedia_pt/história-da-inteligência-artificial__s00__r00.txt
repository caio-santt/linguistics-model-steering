Alguns deles desenvolveram modelos matemáticos que formalizavam a lógica simbólica, enquanto outros começaram a construir máquinas capazes de executar essas regras em tempo real. Em 1943, Warren McCulloch e Walter Pitts publicaram “A Logical Calculus of the Ideas Immanent in Nervous Activity”, um artigo que introduziu o conceito de neurônio artificial e estabeleceu a base para a computação neurológica. Pouco depois, Alan Turing, em 1950, propôs o famoso “Teste de Turing” como um critério para a inteligência de máquinas, propondo que uma máquina poderia ser considerada inteligente se conseguisse enganar um interrogador humano em uma conversa por texto.

O mesmo ano marcou a publicação do relatório do Projeto de Pesquisa em Inteligência Artificial, organizado pelo Instituto de Tecnologia de Massachusetts (MIT), que delineou os objetivos de criar programas capazes de raciocinar, aprender e resolver problemas. Na década de 1950, o linguista Noam Chomsky desenvolveu a teoria da gramática generativa, influenciando o campo da linguagem natural e inspirando pesquisadores a criar algoritmos que pudessem interpretar e gerar texto em linguagem humana.

Durante os anos 1960, o foco da IA se expandiu para além da lógica simbólica, incorporando abordagens probabilísticas e estatísticas. O pioneiro John McCarthy, em 1956, cunhou o termo “Inteligência Artificial” no Workshop de Dartmouth, onde apresentou a visão de que a inteligência poderia ser formalizada e reproduzida por máquinas. McCarthy também introduziu a linguagem LISP, que se tornou a ferramenta de programação dominante em pesquisas de IA devido à sua flexibilidade na manipulação de estruturas simbólicas.

Paralelamente, os pesquisadores de aprendizado de máquina começaram a explorar algoritmos de adaptação automática. O algoritmo de perceptron, desenvolvido por Frank Rosenblatt em 1958, foi o primeiro modelo de rede neural a demonstrar a capacidade de aprender a partir de exemplos, embora limitado a problemas lineares. Em 1967, o algoritmo de “backpropagation” (retropropagação) foi formalizado por Paul Werbos, permitindo o treinamento de redes neurais profundas, embora sua aplicação prática só se tornasse viável décadas depois com o aumento exponencial de poder computacional.

A década de 1970 viu o surgimento de sistemas especialistas, como o MYCIN, projetado para diagnosticar infecções bacterianas. Esses sistemas usavam regras de produção e bases de conhecimento expert para tomar decisões em domínios específicos, demonstrando que a IA poderia ser aplicada em contextos do mundo real. No mesmo período, o campo de “inteligência artificial distribuída” começou a ganhar atenção, com a proposta de que múltiplas máquinas poderiam cooperar para resolver problemas complexos, inspirando o desenvolvimento de redes de sensores e dispositivos interconectados.