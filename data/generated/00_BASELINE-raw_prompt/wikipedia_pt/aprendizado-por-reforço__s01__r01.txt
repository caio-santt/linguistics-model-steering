A partir da formulação clássica do processo de decisão de Markov (MDP), os algoritmos de aprendizado por reforço evoluíram para incorporar diversas técnicas de otimização e aproximação, visando superar limitações impostas pela dimensionalidade e pela ausência de um modelo explícito do ambiente. A primeira etapa conceitual que se destaca é a distinção entre **política** e **valor**: enquanto a política descreve o comportamento do agente em cada estado, o valor quantifica a utilidade esperada de um estado (ou de um estado‑ação) sob uma política específica. Essa dualidade dá origem a dois grandes paradigmas de solução: **policy iteration** e **value iteration**.

Em **policy iteration**, o agente começa com uma política inicial arbitrária, calcula o valor associado a essa política (fase de avaliação), e então atualiza a política de forma a maximizar o valor calculado (fase de melhoria). Esse ciclo continua até que a política se torne estável, ou seja, não haja mais ganhos em termos de valor esperado. A avaliação de política, quando feita de forma exata, envolve a resolução de um sistema linear de tamanho igual ao número de estados, o que rapidamente se torna impraticável em ambientes de alta dimensionalidade. Para contornar essa barreira, os pesquisadores introduziram **policy iteration aproximada**, que utiliza métodos de estimação de valor baseados em amostragem ou em aproximações de função (por exemplo, regressão linear, redes neurais, árvores de decisão). Essa abordagem permite que o agente aprenda políticas em espaços de estado contínuos ou de alta cardinalidade.

Por outro lado, a **value iteration** unifica avaliação e melhoria em um único passo de atualização, recursivamente aplicando a **equação de Bellman**. Em cada iteração, o valor de um estado é atualizado com base na recompensa imediata mais o valor futuro esperado, ponderado pela política atual. Quando a política é derivada diretamente do valor calculado, a value iteration convergirá para a política ótima em MDPs finitos, desde que o fator de desconto seja menor que 1. Contudo, como na policy iteration, a necessidade de armazenar valores para todos os estados torna esse método inviável em cenários de grande escala.

Para superar essas limitações, surgiram algoritmos de **aprendizado por reforço baseado em amostragem**, entre eles o famoso **Q‑learning**. Em Q‑learning, o agente mantém uma tabela Q(s, a) que estima o valor de executar a ação a no estado s e seguir a política ótima a partir daí.