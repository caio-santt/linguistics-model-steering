Em seguida, o agente pode operar em cenários em que a dinâmica do ambiente não está completamente conhecida, usando estimativas baseadas em dados coletados em tempo real. Nessa perspectiva, a construção de modelos de transição probabilísticos torna-se uma ferramenta crucial: a partir de amostras de interações (s, a, s’, r), o agente infere a distribuição condicional P(s’|s, a) e a função de recompensa R(s, a). Quando o modelo é preciso, o agente pode simular futuros caminhos, calculando a utilidade esperada de cada ação e, assim, escolher a que maximiza a soma descontada das recompensas futuras.

Um dos desafios centrais nessa abordagem é o chamado “exploração versus exploração”. Se o agente focar apenas em ações que já demonstraram alta recompensa, ele pode perder oportunidades de descobrir estratégias ainda mais vantajosas. Por outro lado, explorar demais pode levar a uma taxa de aprendizado lenta e a uma acumulação de recompensas menores. Várias técnicas foram desenvolvidas para equilibrar esse dilema, como o uso de políticas ε-greedy, onde com probabilidade ε o agente escolhe uma ação aleatória, e com probabilidade 1‑ε escolhe a ação com maior valor estimado; ou métodos baseados em otimização de confiança, como UCB (Upper Confidence Bound), que adiciona um termo de exploração ao valor previsto.

Outra fronteira interessante é a aplicação de aprendizado por reforço em ambientes de alta dimensionalidade, como jogos de vídeo e sistemas de recomendação. Nessas situações, a função de valor ou a política não podem ser representadas de forma tabular; em vez disso, redes neurais profundas são empregadas como aproximadores de função. A arquitetura Deep Q-Network (DQN) exemplifica essa ideia: a rede recebe como entrada o estado (ou uma representação compacta dele) e produz estimativas dos valores Q(s, a) para todas as ações possíveis. A aprendizagem ocorre por meio de descida de gradiente, minimizando a diferença entre a previsão da rede e a recompensa observada mais o valor máximo estimado do próximo estado (o famoso termo de bootstrapping). Para estabilizar o treinamento, técnicas como replay buffer (armazenamento de experiências passadas) e target network (rede alvo que atualiza lentamente) são empregadas.

Além disso, o conceito de “reward shaping” permite que engenheiros de sistema influenciem o comportamento do agente, adicionando recompensas auxiliares que guiam a aprendizagem sem alterar o objetivo final. Por exemplo, em um robô que deve aprender a caminhar, pode-se fornecer uma recompensa adicional por manter o equilíbrio, facilitando a descoberta de sequências de movimentos que conduzem à locomoção estável.