Alguns deles desenvolveram modelos matemáticos que descreviam a lógica booleana e a lógica de primeira ordem, permitindo que máquinas processassem instruções de forma semelhante ao raciocínio humano. Em 1943, Warren McCulloch e Walter Pitts publicaram “A lógica dos neurônios artificiais”, propondo o primeiro modelo de neurônio binário, que seria a pedra‑fundacional da rede neural artificial. Pouco tempo depois, em 1945, Alan Turing lançou o artigo “Computing Machinery and Intelligence”, onde introduziu o que viria a ser conhecido como Teste de Turing, questionando se uma máquina poderia ser considerada “inteligente” ao enganar um observador humano.

Na década de 1950, John McCarthy, Marvin Minsky, Allen Newell e Herbert A. Simon organizaram a conferência de Dartmouth (1956), considerada o nascimento oficial da IA como disciplina acadêmica. McCarthy cunhou o termo “inteligência artificial” e propôs que computadores pudessem simular qualquer aspecto da inteligência humana. Nesse mesmo período, Newell e Simon criaram o Logic Theorist (1955), o primeiro programa de IA que provava teoremas de lógica, demonstrando que máquinas poderiam realizar tarefas de raciocínio simbólico.

Ao longo dos anos 1960 e 1970, a IA simbólica prosperou. Sistemas especialistas, como o MYCIN (diagnóstico de infecções bacterianas) e o XCON (configuração de sistemas de computadores), mostraram que regras baseadas em conhecimento humano poderiam resolver problemas complexos em domínios específicos. Paralelamente, a pesquisa em redes neurais evoluiu, culminando na publicação do algoritmo de retropropagação em 1986 por Rumelhart, Hinton e Williams, que permitiu o treinamento efetivo de redes multicamadas.

A década de 1990 trouxe uma nova fase, marcada pelo ressurgimento do aprendizado de máquina supervisionado e não supervisionado. O algoritmo de máquinas de vetor de suporte (SVM), introduzido por Cortes e Vapnik, e os métodos de clustering como k‑means, ampliaram as possibilidades de classificação e agrupamento de dados. O mesmo período viu a ascensão do aprendizado profundo, embora ainda em estágios iniciais, com a proposta de redes convolucionais (CNN) por LeCun para reconhecimento de imagens.

Com o advento da internet e a explosão de dados, os anos 2000 e 2010 foram transformados pela capacidade de processar grandes volumes de informação. O algoritmo de aprendizado de máquina de gradiente descendente estocástico (SGD) e as arquiteturas de redes recorrentes (RNN) e LSTM (Long Short‑Term Memory) permitiram avanços significativos em processamento de linguagem natural e reconhecimento de fala.