Na década de 1980, a ascensão dos *expert systems* marcou um novo capítulo na evolução da IA. Sistemas como o MYCIN, desenvolvido pela Stanford Artificial Intelligence Laboratory, demonstraram a capacidade de diagnosticar doenças infecciosas com precisão comparável a especialistas humanos, utilizando regras de produção e inferência lógica. A partir de 1982, a comunidade científica começou a reconhecer que o conhecimento explícito, codificado em regras, podia ser transferido para máquinas, gerando um entusiasmo sem precedentes em relação à aplicação comercial da IA [10].

Contemporaneamente, o ressurgimento das redes neurais artificiais, impulsionado por pesquisas de Geoffrey Hinton e David Parker, trouxe de volta a ideia de aprendizagem baseada em padrões. A técnica de *backpropagation*, popularizada em 1986, permitiu a otimização de redes multicamadas, abrindo caminho para o reconhecimento de padrões em imagens e voz. Embora os recursos computacionais ainda fossem limitados, os resultados promissores levaram a uma renovação do interesse acadêmico e a investimentos públicos em projetos de visão computacional [11].

No mesmo período, a abordagem simbólica enfrentou críticas crescentes por sua incapacidade de lidar com incertezas e dados não estruturados. Em resposta, surgiram métodos híbridos que combinavam redes neurais com lógica fuzzy, buscando aproveitar o melhor dos dois mundos. O algoritmo *Fuzzy Inference System* (FIS), desenvolvido por Lotfi Zadeh, foi adotado em aplicações industriais para controle de processos, demonstrando que a IA poderia operar em ambientes reais com variáveis ambíguas [12].

Enquanto isso, a comunidade de IA continuava a se dividir em torno do debate sobre a viabilidade de criar máquinas verdadeiramente conscientes. Em 1988, o relatório *Artificial Intelligence: A New Synthesis* (AI: A New Synthesis), elaborado por uma coalizão de pesquisadores internacionais, concluiu que a inteligência artificial ainda estava longe de alcançar a consciência humana, mas que os avanços em aprendizado de máquina e processamento de linguagem natural eram inevitáveis [13].

Em 1989, a DARPA lançou o programa *Artificial Intelligence Exploration* (AIX), com o objetivo de acelerar a pesquisa em algoritmos de aprendizado profundo. O programa financiou projetos em universidades e empresas de tecnologia, resultando em inovações como a rede *LeNet*, que introduziu convoluções em redes neurais e foi aplicada com sucesso ao reconhecimento de dígitos manuscritos. Este marco é considerado o precursor das modernas redes convolucionais que dominam o campo da visão computacional [14].

Ao mesmo tempo, a década de 1990 testemunhou a popularização de *machine learning* como subcampo da IA. O algoritmo *Support Vector Machine* (SVM), proposto por Vladimir Vapnik, ganhou destaque por sua capacidade de lidar com dados não lineares e por oferecer garantias teóricas de generalização.