No contexto do Q‑learning, a função de valor‑ação \(Q(s,a)\) evolui por meio de atualizações baseadas no algoritmo de Bellman, ajustando‑se iterativamente até convergir para a solução ótima. Entretanto, quando o espaço de estados ou de ações se torna tão vasto que a tabela de Q‑values não cabe na memória ou requer um tempo de aprendizado impraticável, surgem os métodos de aproximação. Nesses cenários, a função \(Q(s,a;\theta)\) é representada por um modelo paramétrico, como redes neurais artificiais, polinômios ou funções de base radial. A escolha do representador determina a capacidade do agente em generalizar a partir de poucos exemplos, permitindo que ele preveja valores de recompensa para estados nunca antes visitados.

A introdução de redes neurais no aprendizado por reforço deu origem ao que hoje chamamos de Deep Reinforcement Learning (DRL). O algoritmo Deep Q‑Network (DQN) demonstrou, em 2015, que um agente pode aprender a jogar jogos clássicos de Atari apenas observando pixels, superando humanos em alguns casos. O DQN introduziu duas inovações cruciais: a utilização de uma rede neural convolucional para extrair características visuais e o uso de “experience replay”, que armazena experiências em uma memória de amostras e as amostra aleatoriamente durante o treinamento. Isso reduz a correlação temporal entre observações consecutivas, estabilizando o processo de aprendizado. Além disso, o DQN emprega uma rede alvo estática, atualizada periodicamente, para calcular o valor de retorno estimado, mitigando oscilações e divergências.

Para lidar com ambientes com ação contínua, surgiram algoritmos de política paramétrica, como o Policy Gradient e o Actor‑Critic. O algoritmo Proximal Policy Optimization (PPO) tornou-se um padrão de fato, pois combina a simplicidade de métodos de gradiente de política com a estabilidade de técnicas de clipping que evitam atualizações drásticas. Em PPO, a política é otimizada em passos pequenos, mantendo a distância entre a política antiga e a nova dentro de um intervalo seguro, o que reduz o risco de degradação do desempenho. Paralelamente, o Critic estima a função de valor \(V(s;\phi)\), que auxilia na redução da variância do gradiente de política.

A exploração continua sendo um dos maiores desafios em AR. Estratégias clássicas, como \(\epsilon\)-greedy e Boltzmann exploration, foram complementadas por abordagens mais sofisticadas, como Upper Confidence Bound (UCB) e Thompson Sampling. Em cenários de grande escala, métodos de exploração baseados em curiosidade, que incentivam o agente a buscar estados que geram maior surpresa, têm mostrado resultados promissores.