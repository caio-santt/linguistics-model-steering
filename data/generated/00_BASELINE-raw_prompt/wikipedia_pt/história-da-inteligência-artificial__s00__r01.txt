Alguns deles desenvolveram modelos iniciais de máquinas que simulavam processos cognitivos simples, como o *Logic Theorist* de Allen Newell e Herbert A. Simon, que em 1955 demonstrou que uma máquina poderia provar teoremas de lógica proposicional. Em 1956, a conferência de Dartmouth, organizada por John McCarthy, Marvin Minsky, Nathaniel Rochester e Claude Shannon, marcou o nascimento formal da disciplina. McCarthy cunhou o termo “inteligência artificial” e propôs que “qualquer aspecto do aprendizado ou da inteligência pode, em princípio, ser descrito de forma precisa o suficiente para que uma máquina o reproduza”.

A década seguinte foi de experimentação e otimismo. Os primeiros sistemas de “expert” – como o programador de diagnóstico de falhas de bombas de combustível – mostraram que regras explícitas poderiam ser codificadas para resolver problemas específicos. Entretanto, o entusiasmo logo foi temperado pela realidade de que a lógica simbólica, embora poderosa, não conseguia lidar com a incerteza e ambiguidade do mundo real. Foi então que surgiram as primeiras abordagens probabilísticas, como o modelo de Bayes e o algoritmo de “Maximum Likelihood”, que permitiram que as máquinas incorporassem crenças sobre o estado do ambiente e atualizassem essas crenças à medida que novas evidências surgiam.

Nos anos 60, o interesse por redes neurais começou a se ressurgir, inspirado pelos trabalhos de McCulloch e Pitts sobre neurônios artificiais e a posterior descoberta de que redes com apenas duas camadas poderiam aproximar funções não lineares. Entretanto, a falta de recursos computacionais e o “paradoxo de overfitting” impediram avanços significativos. Foi apenas na década de 80, com a introdução de algoritmos de retropropagação e a popularização de computadores pessoais, que as redes neurais ganharam nova vida. Este período viu o surgimento de algoritmos de aprendizado supervisionado que poderiam treinar modelos a partir de exemplos rotulados, marcando a transição da IA simbólica para a IA baseada em dados.

Ao mesmo tempo, a área de “planejamento automático” se consolidou. Sistemas como STRIPS (Stanford Research Institute Problem Solver) demonstraram que algoritmos de busca em grafos poderiam gerar sequências de ações que levassem de um estado inicial a um objetivo desejado. Essa abordagem combinou lógica formal com heurísticas de busca, estabelecendo uma base para a inteligência de jogos e robótica.

A década de 1990 foi marcada por grandes marcos. O algoritmo AlphaGo, desenvolvido pela DeepMind, mostrou que uma combinação de aprendizado profundo e Monte Carlo Tree Search podia vencer jogadores humanos em Go, um jogo considerado mais complexo do que o xadrez.