Continuar o percurso da IA exige reconhecer que, apesar das promessas iniciais, o campo enfrentou décadas de “invernos” e de redescobertas. Nos anos 60, o foco deslocou-se para a lógica simbólica, com o desenvolvimento de sistemas de raciocínio baseados em regras. A lógica de proposições e a lógica de predicados tornaram-se ferramentas centrais para a representação de conhecimento, permitindo que programas como o **Logic Theorist** (1955) e o **General Problem Solver** (1957) fossem formalizados em linguagens de programação de alto nível, como o **LISP** de John McCarthy. Em 1965, o **ELIZA** de Joseph Weizenbaum demonstrou que, embora simples, um programa de processamento de linguagem natural pudesse enganar usuários em conversas superficiais, provocando debates éticos sobre a simulação de inteligência.

Na década de 1970, o avanço tecnológico em hardware possibilitou a introdução de redes de perceptrons, os precursores dos modernos **redes neurais artificiais**. O perceptron de Rosenblatt, embora limitado a problemas linearmente separáveis, inspirou a pesquisa em **aprendizado supervisionado**. Paralelamente, o surgimento de **sistemas especialistas** como o **MYCIN** (1972) demonstrou a viabilidade de encapsular conhecimento médico em regras de inferência, consolidando a ideia de que a IA poderia ser aplicada em domínios específicos de alto valor.

O fim dos anos 70 e o início dos anos 80 viram a emergência do **aprendizado por reforço**. A obra seminal de Sutton e Barto (1988) formalizou o conceito de agente que aprende a maximizar recompensas ao interagir com o ambiente, influenciando algoritmos de controle autônomo em robótica e jogos. Ao mesmo tempo, a **teoria da complexidade computacional** começou a delinear limites teóricos para a IA, destacando que certos problemas, mesmo com recursos ilimitados, permanecem inapropriados para soluções automatizadas.

A década de 1990 marcou a consolidação do **machine learning** como subcampo dominante. O algoritmo **Support Vector Machine** (SVM) de Cortes e Vapnik (1995) introduziu a ideia de maximização da margem, estabelecendo um padrão para classificadores robustos. Em 1997, o **Deep Blue** da IBM derrotou o campeão mundial de xadrez Garry Kasparov, provando que sistemas baseados em busca combinatória e avaliação heurística podiam superar a inteligência humana em tarefas altamente estruturadas. Esse evento, porém, não foi o fim da IA; ele apenas sinalizou que a combinação de poder computacional e algoritmos sofisticados poderia ultrapassar barreiras previamente consideradas intransponíveis.