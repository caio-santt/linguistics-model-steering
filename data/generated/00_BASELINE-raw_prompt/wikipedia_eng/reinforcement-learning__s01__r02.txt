When the agent’s performance is compared to that of a human expert or a handcrafted baseline, the evaluation typically involves a set of test episodes drawn from the same distribution as the training data. The cumulative return over these episodes serves as a proxy for optimality, while the variance of the return reflects the agent’s robustness to stochasticity in the environment. In many practical applications, such as robotic manipulation or autonomous driving, additional metrics—energy consumption, safety violations, or adherence to temporal constraints—are incorporated to provide a multidimensional assessment of the learned policy.

To quantify sample efficiency, researchers often plot learning curves that track the average return as a function of the number of environment interactions. The slope of the curve near the origin indicates how quickly the agent extracts useful information from each experience, whereas the asymptotic plateau reflects the ultimate performance ceiling given the algorithmic design and the expressiveness of the function approximator. In tabular settings, the convergence rate can be bounded analytically using concentration inequalities, but once neural networks replace lookup tables, the theoretical guarantees become elusive, and empirical evaluation gains primacy.

Exploration remains the central challenge in reinforcement learning. The classic ε‑greedy scheme, where the agent selects a random action with probability ε and otherwise follows its current policy, offers simplicity at the cost of potentially suboptimal exploration. More sophisticated approaches, such as Boltzmann exploration, upper‑confidence‑bound methods, or intrinsic motivation signals, aim to balance the trade‑off between exploiting known rewarding actions and discovering new, potentially superior strategies. In continuous action spaces, algorithms like Deep Deterministic Policy Gradient (DDPG) or Proximal Policy Optimization (PPO) incorporate exploration noise in the action selection process, but the choice of noise distribution can dramatically influence learning dynamics.

Function approximation introduces another layer of complexity. While linear models enable convergence proofs under certain conditions, nonlinear approximators such as deep neural networks provide the expressive power needed for high‑dimensional sensory inputs. However, they also bring issues of stability and over‑fitting. Techniques like experience replay, target networks, and entropy regularization have been developed to mitigate catastrophic forgetting and to encourage diverse policy behaviors. In the context of partially observable Markov decision processes (POMDPs), recurrent architectures (e.g., LSTMs or GRUs) are employed to compress history into a hidden state, enabling the agent to infer latent variables that are otherwise inaccessible.

The field has also seen a surge in model‑based reinforcement learning, where the agent learns an explicit dynamics model and plans using that model.