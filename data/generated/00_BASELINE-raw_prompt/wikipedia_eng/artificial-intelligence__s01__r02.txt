Among the many ontological frameworks that emerged in the early 2000s, the Resource Description Framework (RDF) and the Web Ontology Language (OWL) became foundational for encoding domain knowledge in a machine‑readable format. RDF’s triple‑based model—subject, predicate, object—offers a flexible graph structure that supports heterogeneous data sources, while OWL adds richer expressivity through class hierarchies, property restrictions, and cardinality constraints. Together, they enable automated reasoners to infer implicit facts, detect inconsistencies, and support semantic search across distributed knowledge bases [ 29 ].

Despite these advances, knowledge acquisition remains a bottleneck. Human experts are still required to curate ontologies, a process that is labor‑intensive and prone to bias. Recent work on semi‑automated ontology learning leverages statistical natural language processing (NLP) to extract candidate concepts and relations from corpora, subsequently refining them through active learning loops with domain specialists [ 30 ]. These methods reduce manual effort but still struggle with polysemy, synonymy, and context‑dependent meanings, especially in specialized fields such as medicine or law.

In parallel, the rise of probabilistic graphical models—Bayesian networks, Markov random fields, and more recently, probabilistic programming languages—has introduced uncertainty handling into knowledge representation. By assigning probability distributions to predicates, these models capture the inherent vagueness of real‑world knowledge and facilitate robust decision‑making under incomplete information [ 31 ]. However, scaling such models to the size of contemporary knowledge graphs is computationally challenging; inference algorithms often resort to approximate techniques such as variational inference or Monte Carlo sampling, which trade accuracy for tractability.

Another dimension of the knowledge representation problem is temporal reasoning. Real‑world events unfold over time, and static ontologies cannot capture dynamic states. Temporal logics (e.g., Allen’s interval algebra) and event calculus frameworks have been integrated with OWL to model time‑dependent facts, but the resulting reasoning tasks become undecidable in the general case. Practical solutions typically constrain the temporal domain or employ hybrid symbolic‑subsymbolic approaches that approximate temporal dynamics using neural embeddings [ 32 ].

Beyond knowledge representation, AI research has identified several complementary subproblems that must be addressed to achieve robust intelligence. These include perception—extracting structured information from raw sensory data; learning—discovering patterns and generalizing from examples; planning—generating sequences of actions that achieve goals; and language understanding—interpreting and generating natural language in context.