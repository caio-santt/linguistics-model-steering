In a Markov decision process (MDP) the world is described by a finite set of states \(S\), a finite set of actions \(A\), a transition function \(T(s,a,s') = \Pr(s' \mid s,a)\), and a reward function \(R(s,a)\). The agent observes its current state, selects an action, receives a reward, and the system moves to a new state according to \(T\). The goal is to find a policy \(\pi: S \rightarrow A\) that maximizes the expected cumulative reward over time, often discounted by a factor \(\gamma \in [0,1)\). Dynamic programming techniques such as value iteration and policy iteration solve the Bellman optimality equations, yielding the optimal value function \(V^*(s)\) and the corresponding greedy policy \(\pi^*(s) = \arg\max_a \sum_{s'} T(s,a,s') [R(s,a) + \gamma V^*(s')]\).

Real systems rarely present full observability. In a partially observable Markov decision process (POMDP) the agent receives observations \(o \in O\) generated by an observation function \(O(s,a,o) = \Pr(o \mid s,a)\). The agent maintains a belief distribution \(b(s)\) over states, updated via Bayes’ rule after each action and observation. The value function becomes a function over beliefs, \(V(b)\), and the optimal policy selects actions that maximize expected reward over the belief space. Solving POMDPs exactly is PSPACE‑hard; approximate methods such as point‑based value iteration, Monte Carlo sampling, and heuristic search are employed in practice.

When the reward structure or transition dynamics are not known a priori, the agent must learn. Reinforcement learning (RL) treats the environment as a black box and updates value estimates or policies based on sampled trajectories. Model‑free algorithms such as Q‑learning or SARSA estimate action‑value functions \(Q(s,a)\) directly from experience, while policy‑gradient methods optimize a parameterized policy \(\pi_\theta(a|s)\) by ascending the expected return. Model‑based RL, on the other hand, learns an approximate transition model \(\hat{T}\) and reward model \(\hat{R}\), then plans within this learned model, often achieving faster convergence in data‑sparse regimes.

In multi‑agent settings the decision problem becomes a stochastic game.