A Markov decision process (MDP) formalizes this situation by specifying a finite set of states \(S\), a finite set of actions \(A\), a transition probability function \(T(s,a,s')\), and a reward function \(R(s,a)\). The agent’s goal is to maximize the expected cumulative reward over a horizon, often discounted by a factor \(\gamma \in [0,1)\). The optimal policy \(\pi^*\) can be derived by solving the Bellman optimality equations, either by value iteration, policy iteration, or linear programming. In practice, however, the state space is often astronomically large or continuous, rendering exact solutions infeasible.

When the agent cannot observe the true state directly, the framework expands to a partially observable Markov decision process (POMDP). Here, the agent maintains a belief state \(b\), a probability distribution over \(S\), updated via Bayes’ rule after each action and observation. The belief MDP is continuous, and approximate solvers such as point‑based value iteration or Monte‑Carlo sampling are employed. The value function over beliefs is typically convex and piecewise linear, which permits efficient representation by a finite set of alpha‑vectors.

In many real‑world scenarios, the reward function itself is not static. Preferences may shift due to social dynamics, changing regulations, or evolving user feedback. Bayesian reinforcement learning addresses this by placing a prior over the unknown parameters of the MDP (e.g., transition probabilities, rewards) and updating this prior with Bayesian inference as data arrives. The agent then optimizes a policy that trades off exploitation of the current posterior against exploration to reduce uncertainty—a principle formalized in the concept of the Bayesian regret bound.

The exploration‑exploitation dilemma is also captured by information‑theoretic criteria such as the information gain or the expected Kullback‑Leibler divergence between prior and posterior after an action‑observation pair. Algorithms like Thompson sampling, Bayesian upper confidence bound (UCB), or posterior sampling for reinforcement learning (PSRL) sample from the posterior to select actions that are optimistic with respect to the agent’s current knowledge. These methods have provable regret guarantees in stochastic bandit and MDP settings.

When multiple agents interact, the decision problem extends to stochastic games or multi‑agent MDPs. Each agent’s policy influences the joint transition dynamics, and equilibria concepts such as Markov perfect equilibrium or correlated equilibrium become relevant. In cooperative settings, agents may share observations or reward signals, leading to joint belief updates and coordinated policies.