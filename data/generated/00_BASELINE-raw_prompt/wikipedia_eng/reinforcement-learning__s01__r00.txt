When the agent’s performance is compared to a benchmark, it is common to employ a set of evaluation metrics that capture both short‑term gains and long‑term stability. One such metric is the *average reward per episode*, which aggregates the cumulative reward over a fixed horizon and normalizes by the episode length. Another is the *return variance*, which measures the dispersion of returns and provides insight into the reliability of the learned policy. In practice, researchers often plot learning curves that depict these metrics against the number of interactions, thereby visualizing the convergence trajectory of the algorithm.

A critical challenge in reinforcement learning is the trade‑off between *exploration* and *exploitation*. Exploration encourages the agent to try unfamiliar actions, potentially discovering higher‑reward strategies that would remain hidden under a purely greedy policy. Exploitation, on the other hand, leverages the current knowledge to maximize immediate reward. Classic techniques such as ε‑greedy, where a random action is chosen with probability ε and the best known action otherwise, strike a simple yet effective balance. More sophisticated methods, like Upper Confidence Bound (UCB) or Thompson Sampling, incorporate statistical confidence intervals to guide exploration more intelligently.

The formal backbone of most reinforcement learning algorithms lies in the *value function*, which estimates the expected return from a given state (or state‑action pair). Two canonical forms exist: the *state‑value function* V(s), which predicts the return starting from state s and following a particular policy, and the *action‑value function* Q(s,a), which predicts the return starting from state s, taking action a, and thereafter following the policy. Temporal‑difference (TD) learning methods, such as TD(0) and Q‑learning, iteratively update these value estimates based on observed transitions. The update rule for Q‑learning, for instance, is:

\[ Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \big[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1},a') - Q(s_t,a_t) \big], \]

where α is the learning rate and γ is the discount factor. This update incorporates the *bootstrapping* principle: the current estimate is refined using a more recent estimate of the future return.

In environments with large or continuous state spaces, representing the value function explicitly becomes infeasible. Function approximation techniques, such as linear combinations of basis functions or deep neural networks, generalize across states.