Two elements make reinforcement learning powerful: the use of samples to optimize performance, and the use of function approximation to deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations: when the state space is combinatorially large, when the dynamics are stochastic and non‑linear, and when the reward signal is sparse or delayed.

In practice, the most common approach to approximate the optimal value function or policy is to parameterize it with a neural network. The parameters are updated by gradient‑based methods that minimize a loss derived from Bellman errors or from policy gradients. In value‑based methods, the network outputs Q‑values for each action; in policy‑based methods, it outputs a probability distribution over actions. Actor‑critic algorithms combine both perspectives, maintaining a critic that estimates the value function and an actor that updates the policy in the direction suggested by the critic.

Exploration remains a central challenge. If the agent always exploits the current best estimate, it risks getting trapped in sub‑optimal policies. Two popular exploration schemes are ε‑greedy, where a random action is chosen with probability ε, and entropy‑regularized policies, where a term proportional to the policy’s entropy is added to the objective. More sophisticated methods, such as Thompson sampling and Bayesian neural networks, explicitly model uncertainty and sample actions from a posterior distribution, thereby encouraging exploration in regions where the agent is less certain.

When the reward signal is sparse—such as in maze navigation where only reaching the goal yields a positive reward—temporal‑difference learning alone struggles. Hierarchical reinforcement learning addresses this by decomposing the task into sub‑tasks, each with its own reward structure. Options, or temporally extended actions, allow the agent to learn reusable skills that can be composed to solve complex tasks. Intrinsic motivation mechanisms, such as curiosity‑driven exploration, provide internal rewards for visiting novel states, thereby guiding the agent toward informative experiences.

The sample efficiency of RL algorithms has improved dramatically with the advent of deep RL. Techniques like experience replay, where past transitions are stored and reused, break the correlation between consecutive samples and stabilize learning. Prioritized replay further biases the sampling toward transitions with high temporal‑difference error, ensuring that the agent focuses on informative experiences. Off‑policy methods, such as Deep Q‑Network (DQN) and Soft Actor‑Critic (SAC), can learn from data generated by a different policy, enabling more efficient reuse of experience.