2 ] The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process , and they target large Markov decision processes where exact methods become infeasible . [ 3 ] Principles Due to its generality , reinforcement learning is studied in many disciplines , such as game theory , control theory , operations research , information theory , simulation-based optimization , multi-agent systems , swarm intelligence , and statistics . In the operations research and control literature , RL is called approximate dynamic programming , or neuro-dynamic programming . The problems of interest in RL have also been studied in the theory of optimal control , which is concerned mostly with the existence and characterization of optimal solutions , and algorithms for their exact computation , and less with learning or approximation ( particularly in the absence of a mathematical model of the environment ) . Basic reinforcement learning is modeled as a Markov decision process : A set of environment and agent states ( the state space ) , The purpose of reinforcement learning is for the agent to learn an optimal ( or near-optimal ) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards . This is similar to processes that appear to occur in animal psychology . For example , biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements , and interpret pleasure and food intake as positive reinforcements . In some circumstances , animals learn to adopt behaviors that optimize these rewards . This suggests that animals are capable of reinforcement learning . [ 4 ] [ 5 ] A basic reinforcement learning agent interacts with its environment in discrete time steps . At each time step t , the agent receives the current state It then chooses an action from the set of available actions , which is subsequently sent to the environment . The environment moves to a new state that maximizes the expected cumulative reward . Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state ; in this case , the problem is said to have full observability . If the agent only has access to a subset of states , or if the observed states are corrupted by noise , the agent is said to have partial observability , and formally the problem must be formulated as a partially observable Markov decision process . In both cases , the set of actions available to the agent can be restricted . For example , the state of an account balance could be restricted to be positive ; if the current value of the state is 3 and the state transition attempts to reduce the value by 4 , the transition will not be allowed . When the agent 's performance is compared to