26 ] knowledge about knowledge ( what we know about what other people know ) ; [ 27 ] default reasoning ( things that humans assume are true until they are told differently and will remain true even when other facts are changing ) ; [ 28 ] and many other aspects and domains of knowledge . Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge ( the set of atomic facts that the average person knows is enormous ) ; [ 29 ] and the sub-symbolic form of most commonsense knowledge ( much of what people know is not represented as `` facts '' or `` statements '' that they could express verbally ) . [ 16 ] There is also the difficulty of knowledge acquisition , the problem of obtaining knowledge for AI applications . [ c ] Planning and decision-making An `` agent '' is anything that perceives and takes actions in the world . A rational agent has goals or preferences and takes actions to make them happen . [ d ] [ 32 ] In automated planning , the agent has a specific goal . [ 33 ] In automated decision-making , the agent has preferencesâ€”there are some situations it would prefer to be in , and some situations it is trying to avoid . The decision-making agent assigns a number to each situation ( called the `` utility '' ) that measures how much the agent prefers it . For each possible action , it can calculate the `` expected utility '' : the utility of all possible outcomes of the action , weighted by the probability that the outcome will occur . It can then choose the action with the maximum expected utility . [ 34 ] In classical planning , the agent knows exactly what the effect of any action will be . [ 35 ] In most real-world problems , however , the agent may not be certain about the situation they are in ( it is `` unknown '' or `` unobservable '' ) and it may not know for certain what will happen after each possible action ( it is not `` deterministic '' ) . It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked . [ 36 ] In some problems , the agent 's preferences may be uncertain , especially if there are other agents or humans involved . These can be learned ( e.g. , with inverse reinforcement learning ) , or the agent can seek information to improve its preferences . [ 37 ] Information value theory can be used to weigh the value of exploratory or experimental actions . [ 38 ] The space of possible future actions and situations is typically intractably large , so the agents must take actions and evaluate situations while being uncertain of what the outcome will be . A Markov decision