Basic reinforcement learning is modeled as a Markov decision process : A set of environment and agent states ( the state space ) , The purpose of reinforcement learning is for the agent to learn an optimal ( or near-optimal ) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards . This is similar to processes that appear to occur in animal psychology . For example , biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements , and interpret pleasure and food intake as positive reinforcements . In some circumstances , animals learn to adopt behaviors that optimize these rewards . This suggests that animals are capable of reinforcement learning . [ 4 ] [ 5 ] A basic reinforcement learning agent interacts with its environment in discrete time steps . At each time step t , the agent receives the current state It then chooses an action from the set of available actions , which is subsequently sent to the environment . The environment moves to a new state that maximizes the expected cumulative reward . Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state ; in this case , the problem is said to have full observability . If the agent only has access to a subset of states , or if the observed states are corrupted by noise , the agent is said to have partial observability , and formally the problem must be formulated as a partially observable Markov decision process . In both cases , the set of actions available to the agent can be restricted . For example , the state of an account balance could be restricted to be positive ; if the current value of the state is 3 and the state transition attempts to reduce the value by 4 , the transition will not be allowed . When the agent 's performance is compared to that of an agent that acts optimally , the difference in performance yields the notion of regret . In order to act near optimally , the agent must reason about long-term consequences of its actions ( i.e. , maximize future rewards ) , although the immediate reward associated with this might be negative . Thus , reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off . It has been applied successfully to various problems , including energy storage , [ 6 ] robot control , [ 7 ] photovoltaic generators , [ 8 ] backgammon , checkers , [ 9 ] Go ( AlphaGo ) , and autonomous driving systems . [ 10 ] Two elements make reinforcement learning powerful : the use of samples to optimize performance , and the use of function approximation to deal with large environments . Thanks to these two key components , RL can be used in large environments in the following situations