por reforço é estudado em muitas disciplinas , como teoria dos jogos , teoria de controle , pesquisa operacional , teoria da informação , otimização baseada em simulação , sistemas multiagente , inteligência de enxame e estatística . Na pesquisa operacional e na literatura de controle , o AR é chamado de programação dinâmica aproximada ou programação neuro-dinâmica . Os problemas de interesse em AR também são estudados na teoria de controle ótimo , que se preocupa principalmente com a existência e caracterização de soluções ótimas e algoritmos para seu cálculo exato , e menos com aprendizado ou aproximação ( particularmente na ausência de um modelo matemático do ambiente ) . O aprendizado por reforço básico é modelado como um processo de decisão de Markov : - Um conjunto de estados do ambiente e do agente ( o espaço de estados ) , S ; - Um conjunto de ações ( o espaço de ações ) , A , do agente ; - Pa ( s , s′ ) = Pr ( St+1 = s′ | St = s , At = a ) , a probabilidade de transição ( no tempo t ) de estado s para estado s′ sob a ação a . - Ra ( s , s′ ) , a recompensa imediata após a transição de s para s′ sob a ação a . O objetivo do aprendizado por reforço é levar o agente a aprender uma política ótima ( ou próxima do ótimo ) que maximize a função de recompensa ou outro sinal de reforço fornecido pelo usuário , acumulado a partir de recompensas imediatas . Isso é semelhante a processos que parecem ocorrer na psicologia animal . Por exemplo , cérebros biológicos são programados para interpretar sinais como dor e fome como reforços negativos e interpretar prazer e consumo de alimentos como reforços positivos . Em algumas circunstâncias , animais aprendem a adotar comportamentos que otimizam essas recompensas , sugerindo que animais são capazes de aprendizado por reforço . [ 4 ] [ 5 ] Um agente básico de aprendizado por reforço interage com seu ambiente em passos de tempo discretos . A cada passo de tempo t , o agente recebe o estado atual St e a recompensa Rt . Em seguida , escolhe uma ação At dentre as ações disponíveis , que então é enviada ao ambiente . O ambiente passa para um novo estado St+1 e a recompensa Rt+1 associada à transição ( St , At , St+1 ) é determinada . O objetivo de um agente de aprendizado por reforço é aprender uma política : π : S × A → [ 0,1 ] , π ( s , a ) = Pr ( At = a | St = s ) que maximize a recompensa acumulada esperada . Formular o problema como um processo de decisão de Markov pressupõe que o agente observa diretamente o estado atual do ambiente ; nesse caso , diz-se que o problema tem observabilidade